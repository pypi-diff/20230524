# Comparing `tmp/deep_training-0.1.7.post0-py3-none-any.whl.zip` & `tmp/deep_training-0.1.7.post1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,13 +1,13 @@
-Zip file size: 345967 bytes, number of entries: 185
+Zip file size: 346084 bytes, number of entries: 185
 -rw-rw-rw-  2.0 fat       47 b- defN 23-Mar-22 07:43 deep_training/__init__.py
--rw-rw-rw-  2.0 fat      902 b- defN 23-May-22 06:49 deep_training/setup.py
+-rw-rw-rw-  2.0 fat      902 b- defN 23-May-23 08:43 deep_training/setup.py
 -rw-rw-rw-  2.0 fat       55 b- defN 22-Dec-09 05:30 deep_training/cv/__init__.py
 -rw-rw-rw-  2.0 fat      195 b- defN 23-Jan-29 01:07 deep_training/data_helper/__init__.py
--rw-rw-rw-  2.0 fat    18126 b- defN 23-May-22 00:45 deep_training/data_helper/data_helper.py
+-rw-rw-rw-  2.0 fat    18145 b- defN 23-May-23 06:49 deep_training/data_helper/data_helper.py
 -rw-rw-rw-  2.0 fat     3757 b- defN 23-May-22 00:45 deep_training/data_helper/data_module.py
 -rw-rw-rw-  2.0 fat    12121 b- defN 23-Apr-27 00:33 deep_training/data_helper/training_args.py
 -rw-rw-rw-  2.0 fat       70 b- defN 22-Dec-13 03:17 deep_training/nlp/__init__.py
 -rw-rw-rw-  2.0 fat       56 b- defN 22-Nov-10 08:28 deep_training/nlp/layers/__init__.py
 -rw-rw-rw-  2.0 fat      241 b- defN 23-Mar-13 05:48 deep_training/nlp/layers/activate.py
 -rw-rw-rw-  2.0 fat    13271 b- defN 22-Nov-14 00:17 deep_training/nlp/layers/crf.py
 -rw-rw-rw-  2.0 fat     4653 b- defN 22-Dec-12 00:12 deep_training/nlp/layers/handshakingkernel.py
@@ -90,15 +90,15 @@
 -rw-rw-rw-  2.0 fat     5149 b- defN 23-Apr-25 03:34 deep_training/nlp/models/pure_model.py
 -rw-rw-rw-  2.0 fat     3949 b- defN 23-Apr-25 03:34 deep_training/nlp/models/simcse.py
 -rw-rw-rw-  2.0 fat     6022 b- defN 23-Apr-25 03:34 deep_training/nlp/models/span_ner.py
 -rw-rw-rw-  2.0 fat    14454 b- defN 23-Apr-25 03:34 deep_training/nlp/models/spn4re.py
 -rw-rw-rw-  2.0 fat    11383 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tplinker.py
 -rw-rw-rw-  2.0 fat     8157 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tplinkerplus.py
 -rw-rw-rw-  2.0 fat     6624 b- defN 23-Apr-25 03:34 deep_training/nlp/models/transformer.py
--rw-rw-rw-  2.0 fat    26541 b- defN 23-May-19 07:05 deep_training/nlp/models/transformer_base.py
+-rw-rw-rw-  2.0 fat    26596 b- defN 23-May-24 00:37 deep_training/nlp/models/transformer_base.py
 -rw-rw-rw-  2.0 fat     7968 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tsdae_model.py
 -rw-rw-rw-  2.0 fat     9040 b- defN 23-Apr-25 03:34 deep_training/nlp/models/w2ner.py
 -rw-rw-rw-  2.0 fat    16524 b- defN 23-Apr-28 07:32 deep_training/nlp/models/LLaMA/__init__.py
 -rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-13 01:04 deep_training/nlp/models/LLaMA/configuration.py
 -rw-rw-rw-  2.0 fat    19207 b- defN 23-Apr-28 07:32 deep_training/nlp/models/LLaMA_parallel/__init__.py
 -rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-10 00:30 deep_training/nlp/models/LLaMA_parallel/configuration.py
 -rw-rw-rw-  2.0 fat    31627 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/__init__.py
@@ -113,26 +113,26 @@
 -rw-rw-rw-  2.0 fat      123 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v1/__init__.py
 -rw-rw-rw-  2.0 fat     7095 b- defN 23-May-19 07:59 deep_training/nlp/models/lora/v1/configuration.py
 -rw-rw-rw-  2.0 fat    13688 b- defN 23-May-04 00:28 deep_training/nlp/models/lora/v1/lora_wrapper.py
 -rw-rw-rw-  2.0 fat      206 b- defN 23-Apr-11 01:58 deep_training/nlp/models/lora/v2/__init__.py
 -rw-rw-rw-  2.0 fat    13112 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v2/adalora_model.py
 -rw-rw-rw-  2.0 fat    11325 b- defN 23-May-19 07:53 deep_training/nlp/models/lora/v2/configuration.py
 -rw-rw-rw-  2.0 fat    11745 b- defN 23-Apr-18 01:24 deep_training/nlp/models/lora/v2/lora_model.py
--rw-rw-rw-  2.0 fat    10644 b- defN 23-May-09 00:34 deep_training/nlp/models/lora/v2/lora_wrapper.py
+-rw-rw-rw-  2.0 fat    10868 b- defN 23-May-24 00:30 deep_training/nlp/models/lora/v2/lora_wrapper.py
 -rw-rw-rw-  2.0 fat     4889 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v2/save_and_load.py
 -rw-rw-rw-  2.0 fat      467 b- defN 23-Apr-21 04:29 deep_training/nlp/models/moss/__init__.py
 -rw-rw-rw-  2.0 fat     5097 b- defN 23-Apr-23 01:11 deep_training/nlp/models/moss/configuration_moss.py
 -rw-rw-rw-  2.0 fat     6735 b- defN 23-Apr-23 01:05 deep_training/nlp/models/moss/custom_autotune.py
 -rw-rw-rw-  2.0 fat    31079 b- defN 23-Apr-23 02:08 deep_training/nlp/models/moss/modeling_moss.py
 -rw-rw-rw-  2.0 fat    18773 b- defN 23-May-15 06:21 deep_training/nlp/models/moss/quantization.py
 -rw-rw-rw-  2.0 fat    15939 b- defN 23-Apr-24 00:32 deep_training/nlp/models/moss/tokenization_moss.py
 -rw-rw-rw-  2.0 fat      203 b- defN 23-May-04 00:28 deep_training/nlp/models/prompt/__init__.py
 -rw-rw-rw-  2.0 fat    12103 b- defN 23-May-19 07:59 deep_training/nlp/models/prompt/configuration.py
--rw-rw-rw-  2.0 fat    52124 b- defN 23-May-15 06:03 deep_training/nlp/models/prompt/prompt_model.py
--rw-rw-rw-  2.0 fat     3551 b- defN 23-May-04 00:28 deep_training/nlp/models/prompt/save_and_load.py
+-rw-rw-rw-  2.0 fat    52598 b- defN 23-May-24 01:46 deep_training/nlp/models/prompt/prompt_model.py
+-rw-rw-rw-  2.0 fat     3560 b- defN 23-May-24 00:30 deep_training/nlp/models/prompt/save_and_load.py
 -rw-rw-rw-  2.0 fat     1917 b- defN 23-May-04 00:28 deep_training/nlp/models/prompt/utils.py
 -rw-rw-rw-  2.0 fat       54 b- defN 23-May-11 01:00 deep_training/nlp/models/rl/__init__.py
 -rw-rw-rw-  2.0 fat      272 b- defN 23-May-15 00:35 deep_training/nlp/models/rl/modeling.py
 -rw-rw-rw-  2.0 fat    21581 b- defN 23-May-19 04:12 deep_training/nlp/models/rl/modeling_ilql.py
 -rw-rw-rw-  2.0 fat    42065 b- defN 23-May-19 04:12 deep_training/nlp/models/rl/modeling_ppo.py
 -rw-rw-rw-  2.0 fat     8163 b- defN 23-May-19 03:28 deep_training/nlp/models/rl/utils.py
 -rw-rw-rw-  2.0 fat      102 b- defN 22-Nov-22 08:00 deep_training/nlp/models/splinker/__init__.py
@@ -176,12 +176,12 @@
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/scheduler/__init__.py
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:26 deep_training/tfnlp/utils/__init__.py
 -rw-rw-rw-  2.0 fat       55 b- defN 23-Mar-07 01:20 deep_training/utils/__init__.py
 -rw-rw-rw-  2.0 fat     1941 b- defN 23-Mar-07 01:20 deep_training/utils/distributed.py
 -rw-rw-rw-  2.0 fat     1724 b- defN 23-Mar-07 01:22 deep_training/utils/func.py
 -rw-rw-rw-  2.0 fat     5117 b- defN 23-Feb-21 09:01 deep_training/utils/maskedlm.py
 -rw-rw-rw-  2.0 fat    14500 b- defN 23-May-11 00:39 deep_training/utils/trainer.py
--rw-rw-rw-  2.0 fat      608 b- defN 23-May-22 06:50 deep_training-0.1.7.post0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-May-22 06:50 deep_training-0.1.7.post0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       14 b- defN 23-May-22 06:50 deep_training-0.1.7.post0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    17964 b- defN 23-May-22 06:50 deep_training-0.1.7.post0.dist-info/RECORD
-185 files, 1245563 bytes uncompressed, 316863 bytes compressed:  74.6%
+-rw-rw-rw-  2.0 fat      608 b- defN 23-May-24 02:22 deep_training-0.1.7.post1.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-May-24 02:22 deep_training-0.1.7.post1.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       14 b- defN 23-May-24 02:22 deep_training-0.1.7.post1.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    17964 b- defN 23-May-24 02:22 deep_training-0.1.7.post1.dist-info/RECORD
+185 files, 1246344 bytes uncompressed, 316980 bytes compressed:  74.6%
```

## zipnote {}

```diff
@@ -537,20 +537,20 @@
 
 Filename: deep_training/utils/maskedlm.py
 Comment: 
 
 Filename: deep_training/utils/trainer.py
 Comment: 
 
-Filename: deep_training-0.1.7.post0.dist-info/METADATA
+Filename: deep_training-0.1.7.post1.dist-info/METADATA
 Comment: 
 
-Filename: deep_training-0.1.7.post0.dist-info/WHEEL
+Filename: deep_training-0.1.7.post1.dist-info/WHEEL
 Comment: 
 
-Filename: deep_training-0.1.7.post0.dist-info/top_level.txt
+Filename: deep_training-0.1.7.post1.dist-info/top_level.txt
 Comment: 
 
-Filename: deep_training-0.1.7.post0.dist-info/RECORD
+Filename: deep_training-0.1.7.post1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deep_training/setup.py

```diff
@@ -1,15 +1,15 @@
 #! -*- coding: utf-8 -*-
 
 from setuptools import setup, find_packages
 
 ignore = ['test','tests']
 setup(
     name='deep_training',
-    version='0.1.7post0',
+    version='0.1.7post1',
     description='an easy training architecture',
     long_description='torch_training: https://github.com/ssbuild/deep_training.git',
     license='Apache License 2.0',
     url='https://github.com/ssbuild/deep_training',
     author='ssbuild',
     author_email='9727464@qq.com',
     install_requires=['lightning>=2',
```

## deep_training/data_helper/data_helper.py

```diff
@@ -85,20 +85,19 @@
 
 
 
 
 class DataHelper(DataPreprocessHelper):
     def __init__(self,
                  model_args: ModelArguments,
-                 training_args: typing.Optional[TrainingArguments],
-                 data_args: typing.Optional[DataArguments],
+                 training_args: typing.Optional[TrainingArguments] = None,
+                 data_args: typing.Optional[DataArguments] = None,
                  **kwargs):
         super(DataHelper, self).__init__()
 
-
         self.data_process_fn = self.on_data_process
 
         self.train_files = []
         self.eval_files = []
         self.test_files = []
 
         self.tokenizer = None
@@ -370,15 +369,15 @@
         if mode == 'train':
             contain_objs = self.train_files
         elif mode == 'eval' or mode == 'val':
             contain_objs = self.eval_files
         elif mode == 'test' or mode == 'predict':
             contain_objs = self.test_files
         else:
-            raise ValueError('{} invalid ', mode)
+            raise ValueError('{} invalid '.format(mode))
 
         if not input_files:
             logging.info('input_files empty!')
             return
 
         data_args: DataArguments = self.data_args
         for i in range(dupe_factor):
```

## deep_training/nlp/models/transformer_base.py

```diff
@@ -107,18 +107,18 @@
         super(TransformerBase, self).__init__()
         self.config = config
         self.base_model_prefix = None
         self.config_class = None
         self._trainer:  typing.Optional["pl.Trainer"]  = None
 
     def forward(self, *args, **batch):
-        return self.model(*args,**batch)
+        return self.model(*args, **batch)
 
-    def compute_loss(self, *args,**batch) -> tuple:
-        return self.model(*args,**batch)
+    def compute_loss(self, *args, **batch) -> tuple:
+        return self.model(*args, **batch)
 
     def post_init(self):
         return self.model.post_init()
 
     def init_weights(self):
         return self.model.init_weights()
 
@@ -361,22 +361,22 @@
 
     def get_model_lr(self,model=None,lr=None):
         lr = lr if lr is not None else self.config.task_specific_params['learning_rate']
         if model is not None:
             return [(model, lr)]
         return self.model.get_model_lr(model=None,lr=None) if model is None else [(model,self.config.task_specific_params['learning_rate'])]
 
-
-    def compute_loss(self,*args, **kwargs):
-        kwargs.update(dict(args))
+    def compute_loss(self, *args, **kwargs):
+        if len(args):
+            kwargs.update(dict(args))
         return self.model.compute_loss(**kwargs)
 
-
-    def forward(self,*args, **kwargs):
-        kwargs.update(dict(args))
+    def forward(self, *args, **kwargs):
+        if len(args):
+            kwargs.update(dict(args))
         return self.compute_loss(**kwargs)
 
 
     def setup(self, stage: str) -> None:
         if self.backbone is not None:
             setattr(self.backbone, 'trainer', self.trainer)
             setattr(self.backbone, 'estimated_stepping_batches', self.trainer.estimated_stepping_batches)
```

## deep_training/nlp/models/lora/v2/lora_wrapper.py

```diff
@@ -124,14 +124,16 @@
 
 
         lora_config.inference_mode = not is_trainable
         model = cls(model, lora_config, adapter_name)
         model.load_adapter(pretrained_model_name_or_path, adapter_name, **kwargs)
         return model
 
+    def load_weight(self, pretrained_model_name_or_path, adapter_name="default", is_trainable=False, **kwargs):
+        self.load_adapter(pretrained_model_name_or_path, adapter_name, is_trainable=is_trainable, **kwargs)
 
 
     def print_trainable_parameters(self):
         """
         Prints the number of trainable parameters in the model.
         """
         trainable_params = 0
@@ -207,14 +209,15 @@
             lora_config = LORA_TYPE_TO_CONFIG_MAPPING[
                 LoraConfig.from_pretrained(model_id, subfolder=kwargs.get("subfolder", None)).lora_type
             ].from_pretrained(model_id, subfolder=kwargs.get("subfolder", None))
 
             lora_config.inference_mode = not is_trainable
             self.add_adapter(adapter_name, lora_config)
 
+
         # load weights if any
         path = os.path.join(model_id, kwargs["subfolder"]) if kwargs.get("subfolder", None) is not None else model_id
 
         if os.path.exists(os.path.join(path, WEIGHTS_NAME)):
             filename = os.path.join(path, WEIGHTS_NAME)
         else:
             raise ValueError(
```

## deep_training/nlp/models/prompt/prompt_model.py

```diff
@@ -11,15 +11,15 @@
 from torch.nn import MSELoss, CrossEntropyLoss, BCEWithLogitsLoss
 from transformers import PreTrainedModel
 from transformers.modeling_outputs import SequenceClassifierOutput, TokenClassifierOutput
 from transformers.utils import PushToHubMixin
 
 from .configuration import PromptLearningConfig, PromptType, PromptBaseArguments, PROMPT_TYPE_TO_CONFIG_MAPPING, \
     WEIGHTS_NAME, TaskType
-from .save_and_load import get_prompt_model_state_dict
+from .save_and_load import get_prompt_model_state_dict, set_prompt_model_state_dict
 from .utils import _prepare_prompt_learning_config
 from ...layers.prompt.prefix_tuning import PrefixEncoder
 from ...layers.prompt.p_tuning import PromptEncoder
 from ...layers.prompt.prompt_tuning import PromptEmbedding
 from ...layers.prompt.utils import _set_trainable, _set_adapter, \
     TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING, shift_tokens_right
 
@@ -151,14 +151,20 @@
         if prompt_config.task_type not in MODEL_TYPE_TO_PROMPT_MODEL_MAPPING.keys():
             model = cls(model, prompt_config, adapter_name)
         else:
             model = MODEL_TYPE_TO_PROMPT_MODEL_MAPPING[prompt_config.task_type](model, prompt_config, adapter_name)
         model.load_adapter(pretrained_model_name_or_path, adapter_name, **kwargs)
         return model
 
+
+
+    def load_weight(self, pretrained_model_name_or_path, adapter_name="default", is_trainable=False, **kwargs):
+        self.load_adapter(pretrained_model_name_or_path, adapter_name,is_trainable=is_trainable, **kwargs)
+
+
     def _setup_prompt_encoder(self, adapter_name):
         config = self.prompt_config[adapter_name]
         self.prompt_encoder = torch.nn.ModuleDict({})
         self.prompt_tokens = {}
         transformer_backbone = None
         for name, module in self.base_model.named_children():
             for param in module.parameters():
@@ -267,14 +273,21 @@
 
     def forward(self, *args, **kwargs):
         """
         Forward pass of the model.
         """
         return self.get_base_model()(*args, **kwargs)
 
+
+
+    #becouse implementation new forward , so need define compute_loss in this class
+    def compute_loss(self,*args,**kwargs):
+        return self.forward(*args,**kwargs)
+
+
     @contextmanager
     def disable_adapter(self):
         """
         Disables the adapter module.
         """
         try:
             if isinstance(self.prompt_config, PromptLearningConfig):
@@ -300,59 +313,55 @@
         if prompt_config.prompt_type != self.prompt_type:
             raise ValueError(
                 f"Cannot combine adapters with different prompt types. "
                 f"Found {self.prompt_type} and {prompt_config.prompt_type}."
             )
         self.prompt_config[adapter_name] = prompt_config
         self._setup_prompt_encoder(adapter_name)
-
         self.set_additional_trainable_modules(prompt_config, adapter_name)
 
     def set_additional_trainable_modules(self, prompt_config, adapter_name):
         if getattr(prompt_config, "modules_to_save", None) is not None:
             if self.modules_to_save is None:
                 self.modules_to_save = set(prompt_config.modules_to_save)
             else:
                 self.modules_to_save.update(prompt_config.modules_to_save)
             _set_trainable(self, adapter_name)
 
     def load_adapter(self, model_id, adapter_name, is_trainable=False, **kwargs):
-
         if adapter_name not in self.prompt_config:
             # load the config
             prompt_config = PROMPT_TYPE_TO_CONFIG_MAPPING[
                 PromptBaseArguments.from_pretrained(model_id, subfolder=kwargs.get("subfolder", None)).prompt_type
             ].from_pretrained(model_id, subfolder=kwargs.get("subfolder", None))
             if isinstance(prompt_config, PromptLearningConfig) and is_trainable:
                 raise ValueError("Cannot set a prompt learning adapter to trainable when loading pretrained adapter.")
             else:
                 prompt_config.inference_mode = not is_trainable
             self.add_adapter(adapter_name, prompt_config)
 
+
         # load weights if any
         path = os.path.join(model_id, kwargs["subfolder"]) if kwargs.get("subfolder", None) is not None else model_id
 
         if os.path.exists(os.path.join(path, WEIGHTS_NAME)):
             filename = os.path.join(path, WEIGHTS_NAME)
         else:
             raise ValueError(
                 f"Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. "
                 f"Please check that the file {WEIGHTS_NAME} is present at {model_id}."
             )
-
-
         adapters_weights = torch.load(
             filename, map_location=torch.device("cuda" if torch.cuda.is_available() else "cpu")
         )
         # load the weights into the model
-        get_prompt_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
+        set_prompt_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
+
 
 
-        # Set model in evaluation mode to deactivate Dropout modules by default
-        self.eval()
 
     def set_adapter(self, adapter_name):
         """
         Sets the active adapter.
         """
         if adapter_name not in self.prompt_config:
             raise ValueError(f"Adapter {adapter_name} not found.")
@@ -655,17 +664,18 @@
             prompts = prompts.to(inputs_embeds.dtype)
             inputs_embeds = torch.cat((prompts, inputs_embeds), dim=1)
             return self.base_model(inputs_embeds=inputs_embeds, **kwargs)
 
     def generate(self, **kwargs):
         prompt_config = self.active_prompt_config
         self.get_transformer_model().prepare_inputs_for_generation = self.prepare_inputs_for_generation
+        generate_fn = getattr(self.base_model,"generate",self.base_model.model.generate)
         try:
             if not isinstance(prompt_config, PromptLearningConfig):
-                outputs = self.base_model.generate(**kwargs)
+                outputs = generate_fn(**kwargs)
             else:
                 if "input_ids" not in kwargs:
                     raise ValueError("input_ids must be provided for Prompt model generation")
                 # For gpt2 models, we construct postion_ids on the fly by using attention mask, and position ids need to match input_shape.
                 # for prefix tuning, input shape is determined using `input_ids`. Thus we should not expand 'attention_mask' here
                 # for prompt tuning input_ids is not passed but a concatenated input_embeds is passed. Thus attention_mask needs to be of same size of num_virtual_tokens + input_ids
                 if kwargs.get("attention_mask", None) is not None and prompt_config.prompt_type in [
@@ -685,16 +695,16 @@
                     kwargs["position_ids"] = None
                 if kwargs.get("token_type_ids", None) is not None:
                     warnings.warn(
                         "Token type ids are not supported for parameter efficient tuning. Ignoring token type ids"
                     )
                     kwargs["token_type_ids"] = None
 
-                outputs = self.base_model.generate(**kwargs)
-        except:
+                outputs = generate_fn(**kwargs)
+        except Exception:
             self.get_transformer_model().prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation
             raise
         else:
             self.get_transformer_model().prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation
             return outputs
 
     def prepare_inputs_for_generation(self, *args, **kwargs):
@@ -876,17 +886,18 @@
 
     def generate(self, **kwargs):
         prompt_config = self.active_prompt_config
         self.get_transformer_model().prepare_inputs_for_generation = self.prepare_inputs_for_generation
         self.get_transformer_model()._prepare_encoder_decoder_kwargs_for_generation = (
             self._prepare_encoder_decoder_kwargs_for_generation
         )
+        generate_fn = getattr(self.base_model, "generate", self.base_model.model.generate)
         try:
             if not isinstance(prompt_config, PromptLearningConfig):
-                outputs = self.base_model.generate(**kwargs)
+                outputs = generate_fn(**kwargs)
             else:
                 if "input_ids" not in kwargs:
                     raise ValueError("input_ids must be provided for Prompt model generation")
                 if kwargs.get("position_ids", None) is not None:
                     warnings.warn(
                         "Position ids are not supported for parameter efficient tuning. Ignoring position ids."
                     )
@@ -894,15 +905,15 @@
                 if kwargs.get("token_type_ids", None) is not None:
                     warnings.warn(
                         "Token type ids are not supported for parameter efficient tuning. Ignoring token type ids"
                     )
                     kwargs["token_type_ids"] = None
 
                 if prompt_config.prompt_type == PromptType.PREFIX_TUNING:
-                    outputs = self.base_model.generate(**kwargs)
+                    outputs = generate_fn(**kwargs)
                 else:
                     raise NotImplementedError
         except:
             self.get_transformer_model().prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation
             self.get_transformer_model()._prepare_encoder_decoder_kwargs_for_generation = (
                 self.base_model_prepare_encoder_decoder_kwargs_for_generation
             )
```

## deep_training/nlp/models/prompt/save_and_load.py

```diff
@@ -47,34 +47,34 @@
             if any(f"{module_name}.modules_to_save.{adapter_name}" in key for module_name in model.modules_to_save):
                 to_return[key.replace("modules_to_save.", "")] = value
 
     to_return = {k.replace(f".{adapter_name}", ""): v for k, v in to_return.items()}
     return to_return
 
 
-def set_peft_model_state_dict(model, peft_model_state_dict, adapter_name="default"):
+def set_prompt_model_state_dict(model, prompt_model_state_dict, adapter_name="default"):
     """
     Set the state dict of the Peft model.
 
     Args:
-        model ([`PeftModel`]): The Peft model.
-        peft_model_state_dict (`dict`): The state dict of the Peft model.
+        model ([`LoraModel`]): The LoraModel.
+        prompt_model_state_dict (`dict`): The state dict of the Peft model.
     """
     config = model.prompt_config[adapter_name]
     state_dict = {}
     if model.modules_to_save is not None:
-        for key, value in peft_model_state_dict.items():
+        for key, value in prompt_model_state_dict.items():
             if any(module_name in key for module_name in model.modules_to_save):
                 for module_name in model.modules_to_save:
                     if module_name in key:
                         key = key.replace(module_name, f"{module_name}.modules_to_save.{adapter_name}")
                         break
             state_dict[key] = value
     else:
-        state_dict = peft_model_state_dict
+        state_dict = prompt_model_state_dict
 
 
     if isinstance(config, PromptLearningConfig) or config.prompt_type == PromptType.ADAPTION_PROMPT:
         peft_model_state_dict = state_dict
     else:
         raise NotImplementedError
```

## Comparing `deep_training-0.1.7.post0.dist-info/METADATA` & `deep_training-0.1.7.post1.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: deep-training
-Version: 0.1.7.post0
+Version: 0.1.7.post1
 Summary: an easy training architecture
 Home-page: https://github.com/ssbuild/deep_training
 Author: ssbuild
 Author-email: 9727464@qq.com
 License: Apache License 2.0
 Platform: UNKNOWN
 Requires-Dist: lightning (>=2)
```

## Comparing `deep_training-0.1.7.post0.dist-info/RECORD` & `deep_training-0.1.7.post1.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 deep_training/__init__.py,sha256=bhATnUT4VEzwvA8_8IwxspnDRKf32ZgEeHYCN2E5Dd4,47
-deep_training/setup.py,sha256=OS3BUJOK7TVnWjcpREUHLZBc4vIxaADvsTISSFN0khY,902
+deep_training/setup.py,sha256=TowzzPS6bqrUsrQ0wzCfwC4LyFLP3z98e05yyKBtqJ0,902
 deep_training/cv/__init__.py,sha256=J-zlKxMsAfAgoO0vSAzgYJXSuMSJcJ7NKAPKeaeC3TM,55
 deep_training/data_helper/__init__.py,sha256=P8rAMalR6xNepAf-9ldGoOSsEiUtur8Px6gUpTXQhd8,195
-deep_training/data_helper/data_helper.py,sha256=afiDCuNfuHgRzDuGQwlxpmMqU2LSgInUfv9aqIrbJyo,18126
+deep_training/data_helper/data_helper.py,sha256=wjy3ziP894WqWrwJqQb5MFZzyh6aKAbSJ3Tz8yOyp1k,18145
 deep_training/data_helper/data_module.py,sha256=0V38xPpgHJK7gGgffBRyobUMgV2MqqtHKL5y5PQzUaA,3757
 deep_training/data_helper/training_args.py,sha256=XGUXdty0SE6n8xqk6J0lySFvaYSGMVo2zuq6paFQ8sM,12121
 deep_training/nlp/__init__.py,sha256=L4_ltrwpG8mrgN1hZRKimefLHgjhRYyXVtLMFzr1grw,70
 deep_training/nlp/layers/__init__.py,sha256=zbd9GfR02_YVgsTJSXjfyIcQwj8PmG4PscMdA0p6ONI,56
 deep_training/nlp/layers/activate.py,sha256=0q7htFl9Az2fdUjrjv-QMUCE5oenYPVTLZ3lRemIKzA,241
 deep_training/nlp/layers/crf.py,sha256=JTihPuJuBBp83I9UZzVg0wogwwpdJrs0VKtuLPBSCDM,13271
 deep_training/nlp/layers/handshakingkernel.py,sha256=BRJZbEjKM347q8zEMEtJXxXjmqhegmQgqebhqMy4UkI,4653
@@ -89,15 +89,15 @@
 deep_training/nlp/models/pure_model.py,sha256=LD8cYvvRirnP8iMFCyRhsNXRHpZt93Kh2WGoUZoEnFw,5149
 deep_training/nlp/models/simcse.py,sha256=ubVGkeMatDeIUqySV8Tc2TJHvaRKb4p3JOvUTOOhaRo,3949
 deep_training/nlp/models/span_ner.py,sha256=rD0TY2K-zesfRFGfDguqkSAfxHGgbQHG3K5QZ6gc7Zg,6022
 deep_training/nlp/models/spn4re.py,sha256=g_pk3bNqpH41EnzJ77oWPsKvbUwzJfaBYMux5FiEc60,14454
 deep_training/nlp/models/tplinker.py,sha256=PJ9smipeIiA1CDi8xz1gIg2DBWzO5C1B2wITItEsd1A,11383
 deep_training/nlp/models/tplinkerplus.py,sha256=hP4KD3rf2hktfQzHnI7RA2j_2cjk_0G5v6CkbLt1gvQ,8157
 deep_training/nlp/models/transformer.py,sha256=ZuywgLt3HZhsR4sJ4SyZvrYgaVJW8lCn5JH1j_IceXE,6624
-deep_training/nlp/models/transformer_base.py,sha256=HUnSGXV3-f3ce2Mn8uEJUi-amusmxeXvuBHTT8WkAFU,26541
+deep_training/nlp/models/transformer_base.py,sha256=X2eni4yyI0p_Ui4nhzSae22IsZYsFgcy1Rq2GqgC-nA,26596
 deep_training/nlp/models/tsdae_model.py,sha256=lb04RIGkhHhilD-vdkfb8YK9hnTck1nN79WX1Pngbbk,7968
 deep_training/nlp/models/w2ner.py,sha256=z0BortOquZSzmma355wNLz1ofLku_hMb2CjL4KDf-PM,9040
 deep_training/nlp/models/LLaMA/__init__.py,sha256=asn9Wxkl4lG12dRVgxq7Lz4BGCNDaRFL155haVMDNso,16524
 deep_training/nlp/models/LLaMA/configuration.py,sha256=HNzzhIIdR9HBN9Y4Oavv6cGgIf0ExcphwsbVkltJ2ZM,5087
 deep_training/nlp/models/LLaMA_parallel/__init__.py,sha256=4fOhbq0tQOTSH5e3X6XN3PnI6athUR8tsTCn4AUg94Q,19207
 deep_training/nlp/models/LLaMA_parallel/configuration.py,sha256=HNzzhIIdR9HBN9Y4Oavv6cGgIf0ExcphwsbVkltJ2ZM,5087
 deep_training/nlp/models/PaLM/__init__.py,sha256=P1qwWPUycRmZ6I48tov6janJUNpp4L-iMoVN54ykcQw,31627
@@ -112,26 +112,26 @@
 deep_training/nlp/models/lora/v1/__init__.py,sha256=zwGdNKqudVj7c8sMWbmZ9CnnncWXuEapAucWY-VEhLs,123
 deep_training/nlp/models/lora/v1/configuration.py,sha256=0Fw6hHmu0j1DwW-CdrAcT2InCyDCMHTa4SvVFJYizY8,7095
 deep_training/nlp/models/lora/v1/lora_wrapper.py,sha256=c7X2raQW6tEN1stIIBJxoHA87mHEsDJZjcPDl_8D9g0,13688
 deep_training/nlp/models/lora/v2/__init__.py,sha256=2XorjeFlyNuH6xTXiyNO1A8A3P5acBApOjxVv3YEon0,206
 deep_training/nlp/models/lora/v2/adalora_model.py,sha256=iKfKWnW--iY2gmXkMcBv6QWJr9vu-uSll57r1UNvRrY,13112
 deep_training/nlp/models/lora/v2/configuration.py,sha256=ttAveFarHqtPI3XvPcCpAD7OG1KJDNw33XjLuC8T6TQ,11325
 deep_training/nlp/models/lora/v2/lora_model.py,sha256=5k09kzj8bSvU-CQm5GJWtg5isXUEUYPmvKuxdLPc_V4,11745
-deep_training/nlp/models/lora/v2/lora_wrapper.py,sha256=CvjnW80ZB558JiNBioYkIO3LWgujcNdgAyVqNsOQttE,10644
+deep_training/nlp/models/lora/v2/lora_wrapper.py,sha256=v-MxTGDvAGDTIr3wG3xWarj9cGIP0n2EE0wMz3_Cu-4,10868
 deep_training/nlp/models/lora/v2/save_and_load.py,sha256=U7_ZaPm8gpg8gQhZei6UG5KvsJXDtSNZfZk1gWo6nWc,4889
 deep_training/nlp/models/moss/__init__.py,sha256=_dQslDggRX8ZsR6RPTUuBzAHotQt8MPAqZ1-nGULPns,467
 deep_training/nlp/models/moss/configuration_moss.py,sha256=Qqp7anpWGnsotkqd5UOfc9e5zhxgx7j5xetSQUOmhaQ,5097
 deep_training/nlp/models/moss/custom_autotune.py,sha256=O-C9w-hZkcrUgDfK4B1iPtKjHQAZwELNefYhlLABHyc,6735
 deep_training/nlp/models/moss/modeling_moss.py,sha256=Trm6zkUFQo9SkgInIuusSFekRvyLa6x2W6WyRE9CswI,31079
 deep_training/nlp/models/moss/quantization.py,sha256=CwkgE9Qkb8HW9xxQUL_B8k-mV2lHrqJ7eS22pGumpB8,18773
 deep_training/nlp/models/moss/tokenization_moss.py,sha256=Ft7hwLBfYoAqn33anM0sbkvU7GuXJQW8NJ1Ddko_1hk,15939
 deep_training/nlp/models/prompt/__init__.py,sha256=D8B65xX2WyGoV4PBPmc1NujefRcgOz1DUq9zcYbBE2g,203
 deep_training/nlp/models/prompt/configuration.py,sha256=D5zvsx_OKEvN7cPQqCjGobY0t9gU30e87sgcxaXlD3c,12103
-deep_training/nlp/models/prompt/prompt_model.py,sha256=OPLAhHCOA2aTVTADIOSxF2ZivAuEeeiSk42EsrlHU6U,52124
-deep_training/nlp/models/prompt/save_and_load.py,sha256=A0RtSZUp_Cn7A-zWIEZKtTAa_quWhVTTi9AtsWB5VlE,3551
+deep_training/nlp/models/prompt/prompt_model.py,sha256=INAmygOdrmrxpvS4jeRsrm3E1OAjNdQ3lQIaM9wDBHw,52598
+deep_training/nlp/models/prompt/save_and_load.py,sha256=kB0ha-h1FlyQfgmaczg7jYktaFEBYzRKTnoKJcXdKMU,3560
 deep_training/nlp/models/prompt/utils.py,sha256=nvNO26eHmgIuY2WrfX3IyyH6jDwEJmQv7ieZqUA-n-0,1917
 deep_training/nlp/models/rl/__init__.py,sha256=pg2jplYDS8gj_w4iUzDgKNFpklxdtOfw4xcTyjA-3xU,54
 deep_training/nlp/models/rl/modeling.py,sha256=3B_v9D-fwpkXH_5v3mBgE0P7MXEXvP-DT7ZlD9qBVis,272
 deep_training/nlp/models/rl/modeling_ilql.py,sha256=xBbwzzeo9ek3_XhUP387gObzdBxxYUVCyu5-WKzGreM,21581
 deep_training/nlp/models/rl/modeling_ppo.py,sha256=uvlUzg_6m_qELTYNXrOiDE3tXb7dEEfKVP1ABL9HLYg,42065
 deep_training/nlp/models/rl/utils.py,sha256=BgocGot9OAvgWug8Dd6DIg2ipnSva1vX9F_WuJpBqRA,8163
 deep_training/nlp/models/splinker/__init__.py,sha256=QtgnpJa78vAq9bzfjN67NmHU3dXU6WH84jeyZoD1sBs,102
@@ -175,11 +175,11 @@
 deep_training/tfnlp/scheduler/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
 deep_training/tfnlp/utils/__init__.py,sha256=kAmlOWNSpQCHbtT-mAsKGQzQFoWKp2jQf3neCJ0cCRY,53
 deep_training/utils/__init__.py,sha256=JFm7m_LPsS9Oavyxn9rbWqllCmV_zBho19rISlHNX4c,55
 deep_training/utils/distributed.py,sha256=-dhvJ6YHpRxvtZ1_on50IE33fUFW3zKXBKqqK-L1HGM,1941
 deep_training/utils/func.py,sha256=1p8hiQDCyk_gQGKrF7y6Dt66k3jLXSAt2IQeJuHQEl8,1724
 deep_training/utils/maskedlm.py,sha256=o8EB2BbDdh7wdgqz9Oi6SsVr1uBWxV15qfTk2VPjWsU,5117
 deep_training/utils/trainer.py,sha256=F1usofzi1lBVHeieDJ7WWdfd1d0Q7tftktwdJgczlg8,14500
-deep_training-0.1.7.post0.dist-info/METADATA,sha256=197-GY-xariYwK0ZD8RaSKxwBd9pb_W5cy8tiWQD42k,608
-deep_training-0.1.7.post0.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-deep_training-0.1.7.post0.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
-deep_training-0.1.7.post0.dist-info/RECORD,,
+deep_training-0.1.7.post1.dist-info/METADATA,sha256=ygSZI6Upy3a5kpLGs80iAkTsKtx6NaLR7W8hRf_KDCU,608
+deep_training-0.1.7.post1.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+deep_training-0.1.7.post1.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
+deep_training-0.1.7.post1.dist-info/RECORD,,
```

