# Comparing `tmp/k1lib-1.3.8.7-py3-none-any.whl.zip` & `tmp/k1lib-1.3.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,10 +1,10 @@
-Zip file size: 2542544 bytes, number of entries: 84
+Zip file size: 2546106 bytes, number of entries: 84
 -rw-rw-r--  2.0 unx     1435 b- defN 23-Feb-08 00:13 k1lib/__init__.py
--rw-rw-r--  2.0 unx    53793 b- defN 23-Apr-02 08:25 k1lib/_baseClasses.py
+-rw-rw-r--  2.0 unx    53798 b- defN 23-May-23 10:44 k1lib/_baseClasses.py
 -rw-rw-r--  2.0 unx    13494 b- defN 23-May-05 13:41 k1lib/_basics.py
 -rw-rw-r--  2.0 unx     4908 b- defN 23-May-08 18:53 k1lib/_context.py
 -rw-rw-r--  2.0 unx     2866 b- defN 22-Jul-20 04:30 k1lib/_higher.py
 -rw-rw-r--  2.0 unx      948 b- defN 22-Sep-21 23:54 k1lib/_k1a.py
 -rw-rw-r--  2.0 unx    11646 b- defN 23-Jan-14 15:37 k1lib/_learner.py
 -rw-rw-r--  2.0 unx    21467 b- defN 23-May-16 05:55 k1lib/_monkey.py
 -rw-rw-r--  2.0 unx     3571 b- defN 21-Nov-04 18:35 k1lib/_perlin.py
@@ -43,44 +43,44 @@
 -rw-rw-r--  2.0 unx     3465 b- defN 22-Nov-27 08:16 k1lib/callbacks/lossFunctions/shorts.py
 -rw-rw-r--  2.0 unx       45 b- defN 21-Aug-11 18:19 k1lib/callbacks/profilers/__init__.py
 -rw-rw-r--  2.0 unx     5054 b- defN 22-May-15 09:01 k1lib/callbacks/profilers/computation.py
 -rw-rw-r--  2.0 unx     2319 b- defN 22-May-15 08:59 k1lib/callbacks/profilers/io.py
 -rw-rw-r--  2.0 unx     4419 b- defN 22-May-15 09:00 k1lib/callbacks/profilers/memory.py
 -rw-rw-r--  2.0 unx     4215 b- defN 22-May-15 09:03 k1lib/callbacks/profilers/time.py
 -rw-rw-r--  2.0 unx      925 b- defN 22-Nov-16 09:22 k1lib/cli/__init__.py
--rw-rw-r--  2.0 unx    15774 b- defN 23-May-19 19:24 k1lib/cli/_applyCl.py
+-rw-rw-r--  2.0 unx    19471 b- defN 23-May-23 19:51 k1lib/cli/_applyCl.py
 -rw-rw-r--  2.0 unx     8308 b- defN 22-Nov-27 07:16 k1lib/cli/bio.py
 -rw-rw-r--  2.0 unx     4033 b- defN 23-Jan-25 02:02 k1lib/cli/cif.py
 -rw-rw-r--  2.0 unx    19321 b- defN 23-May-17 19:01 k1lib/cli/conv.py
 -rw-rw-r--  2.0 unx    24071 b- defN 23-May-17 03:12 k1lib/cli/filt.py
 -rw-rw-r--  2.0 unx     6672 b- defN 23-Jan-25 02:02 k1lib/cli/gb.py
 -rw-rw-r--  2.0 unx     6348 b- defN 23-Apr-05 15:10 k1lib/cli/grep.py
 -rw-rw-r--  2.0 unx    18351 b- defN 23-May-12 23:10 k1lib/cli/init.py
--rw-rw-r--  2.0 unx    25961 b- defN 23-May-22 06:20 k1lib/cli/inp.py
+-rw-rw-r--  2.0 unx    32491 b- defN 23-May-23 19:53 k1lib/cli/inp.py
 -rw-rw-r--  2.0 unx      623 b- defN 22-Jun-22 10:43 k1lib/cli/kcsv.py
 -rw-rw-r--  2.0 unx     4819 b- defN 22-Aug-11 20:44 k1lib/cli/kxml.py
 -rw-rw-r--  2.0 unx     1915 b- defN 21-Nov-12 16:48 k1lib/cli/mgi.py
--rw-rw-r--  2.0 unx    63021 b- defN 23-May-20 01:45 k1lib/cli/modifier.py
+-rw-rw-r--  2.0 unx    64124 b- defN 23-May-23 00:21 k1lib/cli/modifier.py
 -rw-rw-r--  2.0 unx      694 b- defN 22-Nov-27 07:17 k1lib/cli/mol.py
 -rw-rw-r--  2.0 unx     4038 b- defN 23-May-09 15:53 k1lib/cli/nb.py
 -rw-rw-r--  2.0 unx     3530 b- defN 22-Aug-16 15:08 k1lib/cli/optimizations.py
--rw-rw-r--  2.0 unx    11762 b- defN 23-May-16 18:04 k1lib/cli/output.py
+-rw-rw-r--  2.0 unx    12118 b- defN 23-May-22 17:09 k1lib/cli/output.py
 -rw-rw-r--  2.0 unx     2394 b- defN 23-Jan-25 02:03 k1lib/cli/sam.py
 -rw-rw-r--  2.0 unx    49390 b- defN 23-May-19 19:21 k1lib/cli/structural.py
 -rw-rw-r--  2.0 unx    10399 b- defN 22-Aug-05 01:15 k1lib/cli/trace.py
 -rw-rw-r--  2.0 unx    23319 b- defN 22-Sep-29 07:14 k1lib/cli/typehint.py
 -rw-rw-r--  2.0 unx    21101 b- defN 23-May-17 16:57 k1lib/cli/utils.py
 -rw-rw-r--  2.0 unx       20 b- defN 23-Jan-19 22:00 k1lib/k1ui/__init__.py
 -rw-rw-r--  2.0 unx    61803 b- defN 23-Feb-10 12:10 k1lib/k1ui/main.py
 -rw-rw-r--  2.0 unx       20 b- defN 22-Sep-16 01:12 k1lib/serve/__init__.py
 -rw-rw-r--  2.0 unx    10361 b- defN 23-May-05 16:49 k1lib/serve/main.py
 -rw-rw-r--  2.0 unx      153 b- defN 23-May-05 16:00 k1lib/serve/suffix-dash.py
 -rw-rw-r--  2.0 unx      642 b- defN 23-Feb-13 19:00 k1lib/serve/suffix.py
--rw-rw-r--  2.0 unx  2453826 b- defN 23-Jan-19 22:50 k1lib-1.3.8.7.data/data/k1lib/k1ui/256.model.state_dict.pth
--rw-rw-r--  2.0 unx   304735 b- defN 23-Jan-17 19:16 k1lib-1.3.8.7.data/data/k1lib/k1ui/mouseKey.pth
--rw-rw-r--  2.0 unx    20544 b- defN 23-Mar-19 11:14 k1lib-1.3.8.7.data/data/k1lib/serve/main.html
--rw-rw-r--  2.0 unx     1049 b- defN 23-May-22 06:21 k1lib-1.3.8.7.dist-info/LICENSE
--rw-rw-r--  2.0 unx     3864 b- defN 23-May-22 06:21 k1lib-1.3.8.7.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-May-22 06:21 k1lib-1.3.8.7.dist-info/WHEEL
--rw-rw-r--  2.0 unx        6 b- defN 23-May-22 06:21 k1lib-1.3.8.7.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6704 b- defN 23-May-22 06:21 k1lib-1.3.8.7.dist-info/RECORD
-84 files, 3513844 bytes uncompressed, 2532204 bytes compressed:  27.9%
+-rw-rw-r--  2.0 unx  2453826 b- defN 23-Jan-19 22:50 k1lib-1.3.9.data/data/k1lib/k1ui/256.model.state_dict.pth
+-rw-rw-r--  2.0 unx   304735 b- defN 23-Jan-17 19:16 k1lib-1.3.9.data/data/k1lib/k1ui/mouseKey.pth
+-rw-rw-r--  2.0 unx    20544 b- defN 23-Mar-19 11:14 k1lib-1.3.9.data/data/k1lib/serve/main.html
+-rw-rw-r--  2.0 unx     1049 b- defN 23-May-23 19:53 k1lib-1.3.9.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     3888 b- defN 23-May-23 19:53 k1lib-1.3.9.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-May-23 19:53 k1lib-1.3.9.dist-info/WHEEL
+-rw-rw-r--  2.0 unx        6 b- defN 23-May-23 19:53 k1lib-1.3.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6688 b- defN 23-May-23 19:53 k1lib-1.3.9.dist-info/RECORD
+84 files, 3525543 bytes uncompressed, 2535798 bytes compressed:  28.1%
```

## zipnote {}

```diff
@@ -222,32 +222,32 @@
 
 Filename: k1lib/serve/suffix-dash.py
 Comment: 
 
 Filename: k1lib/serve/suffix.py
 Comment: 
 
-Filename: k1lib-1.3.8.7.data/data/k1lib/k1ui/256.model.state_dict.pth
+Filename: k1lib-1.3.9.data/data/k1lib/k1ui/256.model.state_dict.pth
 Comment: 
 
-Filename: k1lib-1.3.8.7.data/data/k1lib/k1ui/mouseKey.pth
+Filename: k1lib-1.3.9.data/data/k1lib/k1ui/mouseKey.pth
 Comment: 
 
-Filename: k1lib-1.3.8.7.data/data/k1lib/serve/main.html
+Filename: k1lib-1.3.9.data/data/k1lib/serve/main.html
 Comment: 
 
-Filename: k1lib-1.3.8.7.dist-info/LICENSE
+Filename: k1lib-1.3.9.dist-info/LICENSE
 Comment: 
 
-Filename: k1lib-1.3.8.7.dist-info/METADATA
+Filename: k1lib-1.3.9.dist-info/METADATA
 Comment: 
 
-Filename: k1lib-1.3.8.7.dist-info/WHEEL
+Filename: k1lib-1.3.9.dist-info/WHEEL
 Comment: 
 
-Filename: k1lib-1.3.8.7.dist-info/top_level.txt
+Filename: k1lib-1.3.9.dist-info/top_level.txt
 Comment: 
 
-Filename: k1lib-1.3.8.7.dist-info/RECORD
+Filename: k1lib-1.3.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## k1lib/_baseClasses.py

```diff
@@ -379,15 +379,15 @@
             x = int(start) if r.start == -inf else int(r.start)
             while x < r.stop: yield x; x += step
     def __neg__(self): return Domain(*neg(self.ranges), dontCheck=True)
     def __add__(self, domain): return Domain(*(r.copy() for r in join(self.ranges, domain.ranges)), dontCheck=True)
     def __sub__(self, domain): return self + (-domain)
     def __and__(self, domain): return Domain(*intersect(self.ranges, domain.ranges), dontCheck=True)
     def __eq__(self, domain): return self.ranges == domain.ranges
-    def __str__(self): return f"Domain: {', '.join(r for r in self.ranges)}"
+    def __str__(self): return f"Domain: {', '.join(str(r) for r in self.ranges)}"
     def __contains__(self, x): return any(x in r for r in self.ranges)
     def __repr__(self):
         rs = '\n'.join(f"- {r}" for r in self.ranges)
         return f"""Domain:\n{rs}\n\nCan:
 - 3 in d: check whether a number is in this domain or not
 - d1 + d2: joins 2 domains
 - -d: excludes the domain from R
```

## k1lib/cli/_applyCl.py

```diff
@@ -1,84 +1,128 @@
 # AUTOGENERATED FILE! PLEASE DON'T EDIT
+__all__ = ["dummy"]
+def dummy():
+    """Does nothing. Only here for you to checkout the source code"""
+    pass
+from k1lib.imports import *; import hashlib
+def hashF(msg:str) -> str: m = hashlib.sha256(); m.update(f"{msg}".encode()); return k1.encode(m.digest())
+def cpuHash() -> str: return None | cmd("lscpu") | join("\n") | aS(hashF)
+loadTestFn = "~/.k1lib/applyCl_loadTest.pth"
+def load_loadTest(): return cat(loadTestFn, False) | aS(dill.loads) if os.path.exists(os.path.expanduser(loadTestFn)) else dict()
+def good_loadTest(): # whether the underlying architecture has been load-tested before
+    obj = load_loadTest(); return None | applyCl.aS(lambda: cpuHash()) | cut(1) | ~inSet(obj) | shape(0) == 0
+def loadTestS(nodeId, cpus, hash_):
+    with k1.timer() as t1:
+        [nodeId]*cpus*4 | insertIdColumn(begin=False) | applyCl(lambda x: range(300_000_000) | toSum(), pre=True) | deref()
+    with k1.timer() as t2:
+        [nodeId]*cpus*4 | insertIdColumn(begin=False) | applyCl(lambda x: range(30_000_000) | apply(op()+2) | toSum(), pre=True) | deref()
+    return [nodeId, cpus, hash_, t1(), t2()]
+def loadTest():
+    data = None | applyCl.aS(lambda: [os.cpu_count(), cpuHash()] | deref()) | ~apply(lambda x,y: [x,*y]) | deref()
+    a = data | ~applyTh(loadTestS, timeout=3600) | deref()
+    alpha1, alpha2 = a | cut(1, 3, 4) | ~apply(lambda x,y,z: [y, z]) | transpose() | toMax().all() | deref()
+    obj = a | ~apply(lambda idx,cpu,h,t1,t2: [h, (alpha1/(t1), alpha2/(t2)) | toMean()]) | toDict() | deref()
+    obj | aS(dill.dumps) | file(loadTestFn)
+def loadTestGuard(guard=True, rounded=True): # returns Dict[nodeId, cpus]
+    if guard and not good_loadTest():
+        ans = input("""applyCl has not load-tested your system yet, so running
+the requested operation might be unbalanced on the cluster (some
+nodes finish before others, wasting computational time). Would you
+like to perform a load test now? Should take 1-2 minutes. Y/n: """)
+        if ans.lower()[0] == "y": loadTest()
+    data = None | applyCl.aS(lambda: [cpuHash(), os.cpu_count()]) | apply(wrapList(), 0) | joinStreams().all() | deref() # List[nodeId, cpu hash, #cpus]
+    obj = {**data | cut(1, 2) | apply("1", 1) | toDict(), **load_loadTest()}
+    return data | lookup(obj, 1) | ~apply(lambda nodeId,mul,cpu: [nodeId, mul*cpu | (aS(round) if rounded else iden())]) | toDict()
 from k1lib.imports import *
 getFolderSize = ls() | filt(os.path.isdir).split() | apply(lambda x: x | (tryout(0) | getFolderSize)) + apply(os.path.getsize) | toSum().all() | toSum() | deref()
 getFilesInFolder = aS(os.walk) | cut(0, 2) | ungroup() | join(os.sep).all()
 def getIr(base): return None | applyCl.aS(lambda: ls(base) | iden() & apply(lambda x: x | (tryout(0) | (aS(os.path.getsize) if os.path.isfile(x) else getFolderSize))) | transpose() | deref()) | ungroup(False) | insertIdColumn(True) | deref()
 def normalize(d):
     d = d | deref(); s = d | cut(1) | toSum()
     return d | apply(op()/s, 1) | sort(0, False) | deref()
 @lru_cache
-def statsCpu():
-    cpu = None | applyCl.aS(applyCl.cpu) | sort(0, False) | deref()
-    cpuF = None | applyCl.aS(applyCl.cpu) | aS(normalize); cpuF # "cpuF" = cpu fraction. List[nodeId, cpu fraction]
+def statsCpu(nodeIds):
+    cpu = loadTestGuard(False).items() | inSet(nodeIds, 0) | sort(0, False) | deref()
+    cpuF = loadTestGuard(False).items() | inSet(nodeIds, 0) | aS(normalize); cpuF # "cpuF" = cpu fraction. List[nodeId, cpu fraction]
     return [cpu, cpuF]
 @lru_cache
 def statsNodeId(): return applyCl.nodeIds()
 _statsS1 = statsNodeId() | apply(wrapList() | insert(0, False)) | toDict()
-def stats(ir):
-    cpu, cpuF = statsCpu()
+def stats(ir, nodeIds):
+    cpu, cpuF = statsCpu(nodeIds)
     # size fraction. List[nodeId, size fraction]
     sizeF = normalize({**_statsS1, **ir | groupBy(1, True) | apply(cut(2) | toSum(), 1) | toDict()}.items()); sizeF
-    return *statsCpu(), sizeF
-def scores(ir): cpu, cpuF, sizeF = stats(ir); return [cpuF, sizeF] | cut(1).all() | transpose() | ~apply(lambda c,s: s-c) | deref() # negpos
-def score(ir): return scores(ir) | apply(lambda x: abs(x)**2) | toSum() # pos
+    return *statsCpu(nodeIds), sizeF
+def scores(ir, nodeIds): cpu, cpuF, sizeF = stats(ir, nodeIds); return [cpuF, sizeF] | cut(1).all() | transpose() | ~apply(lambda c,s: s-c) | deref() # negpos
+def score(ir, nodeIds): return scores(ir, nodeIds) | apply(lambda x: abs(x)**2) | toSum() # pos
 def move(ir, nA:str, nB:str, idx:int):
     ir1 = ir | deref(); ir1[idx][1] = nB; a = ir1 | pretty() | join('\n'); return ir1
-def optimize(ir):
-    cpu, cpuF, sizeF = stats(ir); scs = scores(ir); a = np.argmax(scs); b = np.argmin(scs)
+def optimize(ir, nodeIds):
+    cpu, cpuF, sizeF = stats(ir, nodeIds); scs = scores(ir, nodeIds); a = np.argmax(scs); b = np.argmin(scs)
     # print(scs, a, b)
-    nodeIds = applyCl.nodeIds() | sort(None, False) | deref()
+    nodeIds = nodeIds | sort(None, False) | deref()
     files = {**nodeIds | apply(wrapList() | insert([], False)) | toDict(), **ir | groupBy(1, True) | toDict()}.items() | sort(0, False) | deref()
     fA = files[a]; fB = files[b] # files A. [nodeId, List[idx, url, size]]
     # print(fA); print(fB)
     nA = nodeIds[a]; nB = nodeIds[b] # A node id
     sA, sB = [fA, fB] | apply(op()[1] | cut(2) | deref() | aS(np.array, dtype=int)) # file sizes A
     # print(f"sA: {sA} {sB}")
     spA = sA.sum() - sA; spB = sB.sum() + sA # sum prime A, array[files]
     sp = spA + spB; sfA = spA/sp; sfB = spB/sp
     cA = cpu[a][1]; cB = cpu[b][1] # cpu A
     c = cA + cB; cfA = cA/c; cfB = cB/c # cpu fraction A
     # print(f"sfA: {sfA}, cfA: {cfA}")
     exp = 5 # intuition says that exp should be even. But that doesn't work. Odd values work tho, but I have no idea why
     idx = fA[1][((sfA-cfA)**exp + (sfB-cfB)**exp).argmin()][0]
     ir2 = move(ir, nA, nB, idx)
-    return ir2, [nA, nB, idx, score(ir2)]
-def traj(ir, maxSteps=20):
-    sc = score(ir); aux = None; auxs = []; irs = []
+    return ir2, [nA, nB, idx, score(ir2, nodeIds | aS(tuple))]
+def traj(ir, maxSteps=20, nodeIds=None):
+    sc = score(ir, nodeIds); aux = None; auxs = []; irs = []
     maxSteps = maxSteps if maxSteps is not None else int(1e10)
     for i in range(maxSteps):
-        ir, aux = optimize(ir)
+        ir, aux = optimize(ir, nodeIds)
         if aux[3] > sc: break
         irs.append(ir); auxs.append(aux); sc = aux[3]
     return irs, auxs
 def collapse(it):
-    a, b = it | rows(0, -1)
-    c = [a[0], b[1], a[2], b[3]]
-    if c[0] == c[1]: return []
-    return [c]
-def traj2(ir, maxSteps=20):
+    a, b = it | rows(0, -1); c = [a[0], b[1], a[2], b[3]]
+    return [] if c[0] == c[1] else [c]
+def traj2(ir, traj): # just looks up the file names really, no processing involved
     idx2FileName = ir | apply(lambda arr: [arr[0], arr[2]]) | toDict()
-    a = traj(ir, maxSteps)[1] | groupBy(2) | filt(lambda x: len(x) > 1).split() | (apply(collapse)) + iden() | joinStreams(2) | deref()
+    a = traj | groupBy(2) | filt(lambda x: len(x) > 1).split() | (apply(collapse)) + iden() | joinStreams(2) | deref()
     return a | lookup(idx2FileName, 2) | deref()
 def moveFile(fileName:str, destNodeId:str, timeout=60):
     """Moves file from the current node to the destination node. Usually executed on other nodes than the driver node"""
     fn = os.path.expanduser(fileName); dirname = os.path.dirname(fn)
     [destNodeId] | applyCl.aS(lambda: None | cmd(f"mkdir -p {dirname}; rm -f {fn}") | deref(), timeout=timeout) | deref()
     for chunk in cat(fn, False, True): [destNodeId] | applyCl.aS(lambda: chunk >> file(fn), timeout=timeout) | deref()
     None | cmd(f"rm {fn}") | deref()
 def moveFF(ff:str, destNodeId:str, timeout=60):
     """Moves file or folder from the current node to the destination node"""
     if os.path.isfile(ff): return moveFile(ff, destNodeId, timeout)
     ff | getFilesInFolder | apply(aS(moveFile, destNodeId, timeout)) | deref()
     None | cmd(f"rm -rf {ff}") | ignore()
-def balanceFolder(base, audit=False, maxSteps=20): # currently executing each move step serially, will change in the future if it's too slow
-    applyCl.cmd(f"mkdir -p {base}")
-    tr = traj2(getIr(base), maxSteps)
+def balanceFolder(base, audit=False, maxSteps=1000, timeout=60): # currently executing each move step serially, will change in the future if it's too slow
+    loadTestGuard(); applyCl.cmd(f"mkdir -p {base}")
+    ir = getIr(base); tr = traj2(ir, traj(ir, maxSteps, tuple(applyCl.nodeIds()))[1])
     if audit: return tr
-    tr | apply(lambda arr: [arr[0], arr]) | ~applyCl(lambda a,b,fn,sc: moveFF(fn, b), pre=True, timeout=60) | deref()
+    tr | apply(lambda arr: [arr[0], arr]) | ~applyCl(lambda a,b,fn,sc: moveFF(fn, b), pre=True, timeout=timeout) | deref()
+def decommissionFolderTraj(base:str, nAs:List[str]): # nAs are the ones to decommission
+    ir = getIr(base); targetNodes = applyCl.nodeIds() | ~inSet(nAs) | repeatFrom(); irs = []; auxs = []
+    seis = ir | inSet(nAs, 1) | cut(0, 1) | ~apply(lambda idx, startNode: [startNode, next(targetNodes), idx, 0]) | deref() # List[start, end, index, score]
+    for sei in seis: ir = move(ir, *sei[:3]); irs.append(ir); auxs.append(sei)
+    return irs, auxs
+def decommissionFolder(base:str, nAs:List[str], audit=False, maxSteps=1000, timeout=60):
+    loadTestGuard(); irs, auxs = decommissionFolderTraj(base, nAs)
+    irs2, auxs2 = traj(irs[-1] if len(irs) > 0 else getIr(base), maxSteps, applyCl.nodeIds() | ~inSet(nAs) | aS(tuple))
+    irs = [*irs, *irs2]; auxs = [*auxs, *auxs2]
+    if len(irs) == 0: return
+    tr = traj2(irs[-1], auxs)
+    if audit: return tr
+    tr | apply(lambda arr: [arr[0], arr]) | ~applyCl(lambda a,b,fn,sc: moveFF(fn, b), pre=True, timeout=timeout) | deref()
 def getSize(url):
     for i in range(10):
         try: return requests.head(url, timeout=3).headers.items() | apply(op().lower(), 0) | toDict() | op()["content-length"].ab_int()
         except Exception as e:
             if i == 9: raise Exception(f"Can't get size of file")
 class NoPartialContent(Exception): pass
 def getChunk(url:str, sB:int, eB:int, timeout:float) -> bytes:
@@ -94,15 +138,15 @@
     chunkSize = chunkSize or settings.cli.cat.chunkSize
     return range(sB, eB+1) | batched(chunkSize, True) | apply(lambda r: getChunk(url, r.start, r.stop-1, chunkTimeout))
 def download(url:str, folder:str, merge:bool=False, timeout=120, chunkTimeout=5):
     getChunk(url, 0, 1, 10) # try to see if server accepts partial downloads first
     folder = os.path.expanduser(folder); dirname = os.path.dirname(folder)
     if merge: destFile = folder; folder = b"" | file(); None | cmd(f"rm -rf {folder}") | ignore(); None | cmd(f"mkdir -p {folder}") | ignore()
     applyCl.cmd(f"rm -rf {folder}"); applyCl.cmd(f"mkdir -p {folder}"); size = getSize(url)
-    cpus = None | applyCl.aS(lambda: applyCl.cpu()) | deref(); n = cpus | cut(1) | toSum()
+    cpus = loadTestGuard().items() | deref(); n = cpus | cut(1) | toSum()
     tasks = [cpus | ~apply(lambda x,y: [x]*y) | joinStreams(), range(size) | splitW(*[1]*n)] | transpose() | insertIdColumn(True, False) | ~apply(lambda x,y,z: [x,[y,f"{folder}/{z}.bin"]]) | deref(1)
     tasks | ~applyCl(lambda r, fn: getChunks(url, r.start, r.stop-1, None, chunkTimeout) | file(fn), pre=True, timeout=timeout) | deref()
     if merge:
         None | cmd(f"rm -rf {destFile}") | deref()
         None | cmd(f"mkdir -p {dirname}") | deref()
         None | applyCl.aS(lambda: ls(folder)) | ungroup() | deref() | sortF(op().split(".bin")[0].split("/")[-1].ab_int(), 1) | applyCl(cat(text=False), pre=True) | cut(1) | file(destFile)
         None | cmd(f"rm -rf {folder}") | deref()
@@ -126,15 +170,15 @@
 to decomission nodeAs. The 2 sets should be mutually exclusive
 
 :param rS: instance of refineSeek"""
     nodeAs, nodeBs = [nodeAs, nodeBs] | deref()
     if len(nodeAs) == 0: return
     if len(nodeBs) == 0: raise Exception("Unsupported configuration! Trying to move data from A+B to C+D. Has to have some shared nodes, like moving data from A+B+C to B+C+D. This is not a fundamental limitation, but just can't be done with the current architecture. Might be fixed in the future.")
     # some initial metadata
-    nodeIds = applyCl.nodeIds(); nodeId_cpu = nodeIds | applyCl.aS(lambda: applyCl.cpu()) | deref(); nodeId2Cpu = nodeId_cpu | toDict()
+    nodeIds = applyCl.nodeIds(); nodeId_cpu = loadTestGuard(False).items() | deref(); nodeId2Cpu = nodeId_cpu | toDict()
     ws = nodeId_cpu | inSet(nodeBs, 0) | cut(1) | deref() # weights to split files on nodeAs into
     # splitting file on nodeAs into chunks first, to plan things out
     a = nodeAs | applyCl.aS(lambda: fn | splitSeek(ws=ws) | rS | window(2) | deref() | insertColumn(nodeBs) | insert(applyCl.nodeId()).all() | deref()) | cut(1) | joinStreams() | deref()
     # actually transferring chunks
     with ray.progress(a | groupBy(1) | shape(0), "Decommissioning") as rp:
         c = b = a | groupBy(1, True) | apply(iden() + apply(lambda arr: [arr[0], arr[1:]]) | reverse() | insert(fn)) | deref()
         enumerate(c) | applyTh(~aS(lambda idx, e: a_transfer(*e, rpF=aS(lambda p: ray.get(rp.update.remote(idx, p))))), timeout=None) | deref()
@@ -144,30 +188,30 @@
     """Spreads out a file from nodes A to B, where B fully contains A (no decomissioning).
 A and B should be mutually exclusive. Initial nodes are A, final nodes are A + B"""
     nAs, nBs = [nAs, nBs] | deref(); rS.fn = fn
     if len(nBs) == 0: return # no need to spread out
     nBs | applyCl.aS(lambda: None | cmd(f"mkdir -p {os.path.dirname(fn)}") | deref(), timeout=None) | deref()
     nBs | applyCl.aS(lambda: None | cmd(f"rm -rf {fn}") | deref(), timeout=None) | deref()
     # some initial metadata
-    nodeIds = applyCl.nodeIds(); nodeId_cpu = nodeIds | applyCl.aS(lambda: applyCl.cpu()) | deref(); nodeId2Cpu = nodeId_cpu | toDict()
+    nodeIds = applyCl.nodeIds(); nodeId_cpu = loadTestGuard(False).items() | deref(); nodeId2Cpu = nodeId_cpu | toDict()
     sizes = nAs | applyCl.aS(lambda: os.path.getsize(fn) if os.path.exists(fn) else 0) | deref(); totalSize = sizes | cut(1) | toSum()
     ns = [*nAs, *nBs]; totalCpu = ns | lookup(nodeId2Cpu) | toSum(); bytePerCpu = totalSize/totalCpu; wsB = nBs | lookup(nodeId2Cpu) | deref()
     # prepares segments and metadata, List[nodeId, [sB, eB]], where sB and eB are the ranges of nAs that they're willing to share
     sizePost = sizes | ~apply(lambda idx, size: [idx, nodeId2Cpu[idx]/totalCpu*totalSize/size]) | deref()
     invalidNodes = sizePost | ~filt(lambda x: 0 <= x <= 1, 1) | cut(0) | deref()
     if len(invalidNodes) > 0: raise Exception(f"Unsupported configuration! These nodes have too little data to share: {invalidNodes}. This couldn't have happen using applyCl alone. Data is not corrupted, but you'll have to combine data from all files into 1 and spread them back out again.")
     inter = sizePost | ~apply(lambda idx, x: [idx, [x, 1-x]]) | applyCl(lambda ws: fn | splitSeek(ws=ws) | rS | ~head(1), pre=True) | deref() | filt(~aS(lambda x,y: y-x>0), 1) | deref()
     # actually transferring data to new nodes
     meta = inter | apply(~aS(range) | splitW(*wsB) | rS | apply(wrapList()) | insertColumn(nBs) | deref(1), 1) | ungroup(False) | apply("[x.start, x.stop]", 2) | groupBy(1, True) | deref()
     with ray.progress(len(meta), "Transferring data to new nodes") as rp:
         meta | insertIdColumn(True) | applyTh(~aS(lambda idx, nB, nse: a_transfer(fn, nse, nB, rpF=aS(lambda p: ray.get(rp.update.remote(idx, p))))), timeout=None) | deref()
     # truncates the files in nAs nodes
     inter | ~apply(lambda idx,se: [idx,se[0]]) | applyCl(lambda sB: open(fn, 'a').truncate(sB), pre=True, timeout=None) | deref()
 def balanceFile(fn:str, nAs:List[str]=None, nBs:List[str]=None, rS=iden()):
-    fn = os.path.expanduser(fn)
+    loadTestGuard(); fn = os.path.expanduser(fn)
     if nAs is None: nAs = None | applyCl.aS(lambda: os.path.exists(fn)) | filt(op(), 1) | cut(0) | deref()
     if nBs is None: nBs = applyCl.nodeIds()
     decommission(fn, *nAs | inSet(nBs).split() | reverse(), rS)
     spreadOut(fn, *nBs | inSet(nAs).split(), rS)
 def diskScan1(base:str) -> List[str]: # like ls(), but returns files and folders that appear at least on 2 nodes
     isdir, base = base.split("\ue000")
     if not isdir: return []
@@ -185,15 +229,16 @@
 def diskScan4(base:str, sortSize=True): # fully featured data
     folders, files = diskScan3(base)
     folders = [folders, None | applyCl.aS(lambda: folders | apply(lambda x: (x | getFolderSize) if os.path.exists(x) else 0) | deref()) | cut(1) | transpose()] | transpose() | deref()
     files   = [files,   None | applyCl.aS(lambda: files   | apply(lambda x: os.path.getsize(x)  if os.path.exists(x) else 0) | deref()) | cut(1) | transpose()] | transpose() | deref()
     post = apply(~sortF(toSum(), 1)) if sortSize else iden()
     return [folders, files] | wrapList() + filt(filt(op() > 0) | count() | shape(0) | (op() == 1), 1).split() | joinStreams() | apply(unique(0)) | post | deref()
 def diskScan5(base:str, sortSize=True): # displays it in a nice format
-    d4 = diskScan4(base, sortSize); nodeNames = None | applyCl.aS(lambda: applyCl.cpu()) | apply(op()[:5], 0) | apply('f"{x} thr"', 1) | join(", ").all() | deref(); nodeNames
+    d4 = diskScan4(base, sortSize); lens = d4 | apply(len) | deref(); nodeNames = None | applyCl.aS(lambda: os.cpu_count()) | apply(op()[:5], 0) | apply('f"{x} thr"', 1) | join(", ").all() | deref(); nodeNames
     d5 = d4 | apply(~apply(lambda path, sizes: [path, sizes | toSum() | aS(fmt.size), sizes | apply(fmt.size)]) | insert(["-"*40, "-"*10, ["-"*12]*len(nodeNames)]) | insert(["", "", nodeNames])) | deref(); d5
     ws = d5 | shape(0).all() | deref()
     d6 = d5 | joinStreams() | cut(0, 1) & (cut(2) | pretty() | wrapList().all()) | transpose() | joinStreams().all() | splitW(*ws) | insert(["Path", "Total size", "Size on each node (node id and thread count)"]).all() | joinStreams() | pretty() | splitW(*ws | apply(op()+1)) | deref()
     explainers = ["\nA distributed folder is a folder that has many files and folders inside, but their names\nare all different from each other. It's managed by applyCl.balanceFolder()",
                   "\nA replicated file is a file that has been copied to multiple nodes. Size of all file\ncopies should be the same. It's managed by applyCl.replicateFile()",
                   "\nA distributed file is a file that has been split into multiple pieces and sent to other\nnodes. It's managed by applyCl.balanceFile()"]
-    [d6, ["Distributed folders", "Replicated files", "Distributed files"] | (aS(lambda x: [["-"*60, x, "-"*60] | join(" ")])).all()] | transpose() | permute(1, 0) | (joinStreams() | join("\n")).all() | wrapList() | insert(explainers, False) | transpose() | join("\n").all() | join("\n"*2) | wrapList() | stdout()
+    arr = [d6, ["Distributed folders", "Replicated files", "Distributed files"] | (aS(lambda x: [["-"*60, x, "-"*60] | join(" ")])).all()] | transpose() | permute(1, 0) | (joinStreams() | join("\n")).all() | wrapList() | insert(explainers, False) | transpose() | join("\n").all() | deref()
+    [arr, lens] | transpose() | filt(op(), 1) | cut(0) | join("\n"*2) | wrapList() | stdout()
```

## k1lib/cli/inp.py

```diff
@@ -1,62 +1,164 @@
 # AUTOGENERATED FILE! PLEASE DON'T EDIT
 """This module for tools that will likely start the processing stream."""
-from typing import Iterator, Union, Any
-import k1lib, urllib, subprocess, warnings, os, k1lib, threading, time, warnings, math, io, dill, urllib
+from typing import Iterator, Union, Any, List
+import k1lib, urllib, subprocess, warnings, os, k1lib, threading, time, warnings, math, io, dill, urllib, validators
+from collections import deque
 from k1lib.cli import BaseCli; import k1lib.cli as cli
 from k1lib.cli.typehint import *
+from contextlib import contextmanager
+requests = k1lib.dep("requests")
 try: import minio; hasMinio = True
 except: hasMinio = False
 __all__ = ["cat", "splitSeek", "refineSeek", "curl", "wget", "ls", "cmd", "walk", "requireCli", "urlPath"]
 settings = k1lib.settings.cli
+class NoPartialContent(Exception): pass
+def getChunk(url:str, sB:int, eB:int, timeout:float) -> bytes: # start amd end inclusive
+    for i in range(10):
+        try: res = requests.get(url, headers={"Range": f"bytes={sB}-{eB}"}, timeout=timeout)
+        except Exception as e:
+            if i == 9: raise Exception(f"Can't get file chunk")
+            continue
+        if res.status_code != 206: raise NoPartialContent(f"Server doesn't allow partial downloads at this particular url. Status code: {res.status_code}")
+        return res.content
+def getChunks(url:str, sB:int, eB:int, chunkSize=None, chunkTimeout:float=10) -> List[bytes]:
+    """Grabs bytes from sB to eB in chunks"""
+    chunkSize = chunkSize or settings.cli.cat.chunkSize
+    return range(sB, eB+1) | cli.batched(chunkSize, True) | cli.apply(lambda r: getChunk(url, r.start, r.stop-1, chunkTimeout))
 catSettings = k1lib.Settings().add("chunkSize", 100000, "file reading chunk size for binary+chunk mode. Decrease it to avoid wasting memory and increase it to avoid disk latency")
 catSettings.add("every", k1lib.Settings().add("text", 1000, "for text mode, will print every n lines").add("binary", 10, "for binary mode, will print every n 100000-byte blocks"), "profiler print frequency")
 settings.add("cat", catSettings, "inp.cat() settings")
+
+rfS = k1lib.Settings()
+settings.add("RemoteFile", rfS, "inp.RemoteFile() settings, used in cat(), splitSeek() and the like")
+rfS.add("memoryLimit", 100_000_000, "if the internal cache exceeds this limit (in bytes), and randomAccess is False, then old downloaded chunks will be deleted")
+def noPartial(url):
+    try: return len(getChunk(url, 0, 10, 10)) != 10
+    except NoPartialContent: return True
+class RemoteFile:
+    def __init__(self, url, randomAccess=True, blockSize=None, noPartialConfirm=False):
+        """
+:param url: url of the remote file
+:param randomAccess: is random accessing parts of the file expected? If
+    True, then keeps all of the reads in ram internally, else free them
+    as soon as possible
+:param blockSize: all reads will fetch roughly this amount of bytes"""
+        self.url = url; self.randomAccess = randomAccess; self.blockSize = blockSize or settings.cat.chunkSize
+        self.noPartialConfirm = noPartialConfirm; self.size = None; self.domain = k1lib.Domain()
+        self.seekPos = 0; self.reads = deque() # List[sB, eB, content]
+        self._totalReadSize = 0; self.noPartial = noPartial(url)
+    def _fetch(self, sB:int, eB:int): # fetches from start to end byte and dumps to internal memory. Inclusive start byte, exclusive end byte
+        if not noPartial:
+            eB = max(eB, min(sB+self.blockSize, len(self)))
+            chunk = getChunk(self.url, sB, eB, 10)
+        else:
+            if self.noPartialConfirm:
+                ans = input(f"Remote file '{self.url}' don't support partial downloads. Therefore the entire file will be loaded into RAM, which is may be undesireable. Do you want to continue? Y/n: ")
+                if ans.lower()[0] != "y": self.reads.append([0, 0, b""]); return
+            sB = 0; chunk = requests.get(self.url).content; eB = len(chunk)
+        self.reads.append([sB, eB, chunk])
+        self._totalReadSize += len(chunk); self.domain = self.domain + k1lib.Domain([sB, eB])
+        if not self.randomAccess and self._totalReadSize > rfS.memoryLimit: # deletes old reads
+            sB, eB, chunk = self.reads.popleft()
+            self._totalReadSize -= len(chunk)
+    def _ensureRange(self, sB, eB): # makes sure that all ranges between sB and eB are available
+        missingDomain = k1lib.Domain([max(sB-30, 0), min(eB+30, len(self))]) & -self.domain
+        for sB, eB in missingDomain.ranges: self._fetch(sB, eB)
+    def _readChunks(self, sB, eB): # read from sB to eB, but in chunks, to be optimized. inclusive sB, exclusive eB
+        sB = max(min(sB, len(self)), 0); eB = max(min(eB, len(self)), 0); self._ensureRange(sB, eB)
+        return self.reads | cli.filt(~cli.aS(lambda s,e,chunk: e>=sB and s<=eB)) | cli.sort() | ~cli.apply(lambda s,e,chunk: chunk[max(sB-s, 0):len(chunk)+min(eB-e, 0)])
+    def seek(self, cookie, whence=0):
+        if whence == 0: self.seekPos = cookie
+        elif whence == 1: self.seekPos += cookie
+        elif whence == 2: self.seekPos = len(self) + cookie
+        else: raise Exception("Invalid whence")
+        return self.seekPos
+    def read(self, size, join=True):
+        chunks = self._readChunks(self.seekPos, self.seekPos + size)
+        self.seekPos += size; return b"".join(chunks) if join else chunks
+    def readline(self, newLine=True):
+        ans = []; seekPos = self.seekPos
+        try:
+            while self.seekPos < len(self):
+                for chunk in self.read(self.blockSize, False):
+                    if len(chunk) == 0: raise SyntaxError()
+                    ans.append(chunk)
+                    if b"\n" in chunk: raise SyntaxError()
+        except SyntaxError: pass
+        ans = b"".join(ans)
+        try: n = ans.index(b"\n")
+        except ValueError: n = len(ans)
+        self.seekPos = seekPos + n+1; return (ans[:n+1] if newLine else ans[:n]).decode()
+    def readlines(self, newLine=True):
+        while True:
+            chunk = self.readline(newLine); yield chunk
+            if self.seekPos > len(self): break
+    def tell(self): return self.seekPos
+    def _getSize(self):
+        if self.noPartial: self._fetch(0, 10); return self.reads[0][1]
+        for i in range(10):
+            try: return requests.head(self.url, timeout=3).headers.items() | cli.apply(cli.op().lower(), 0) | cli.toDict() | cli.op()["content-length"].ab_int()
+            except Exception as e:
+                if i == 9: raise Exception(f"Can't get size of remote file: {e}")
+    def __len__(self):
+        if self.size is None: self.size = self._getSize()
+        return self.size
+    def __repr__(self): return f"<RemoteFile url={self.url} size={k1lib.fmt.size(len(self))}>"
+@contextmanager
+def openFile(fn, text, noPartialConfirm=False): # can be actual file or url
+    if os.path.exists(fn):
+        if text:
+            with open(fn, "r") as f: yield f
+        else:
+            with open(fn, "rb") as f: yield f
+    elif validators.url(fn) is True:
+        yield RemoteFile(fn, False, noPartialConfirm=noPartialConfirm)
+    else: raise FileNotFoundError(f"The file {fn} doesn't seem to exist and/or it's not a valid url")
 def _catGenText(fn, sB, eB): # fn for "file name"
     try:
         if sB == 0 and eB == -1: # fast path without bounds (90-160 MB/s expected)
-            with open(fn) as f:
+            with openFile(fn, True) as f:
                 while True:
                     line = f.readline()
                     if line == "": return
                     yield line[:-1] if line[-1] == "\n" else line
         else: # slow path with bounds (15 MB/s expected). Update: much faster now, expect only 40% slower than the path above
             sB = wrap(fn, sB); eB = wrap(fn, eB)
-            with open(fn) as f:
+            with openFile(fn, True) as f:
                 f.seek(sB); b = sB # current byte
                 while True:
                     line = f.readline(); b += len(line)
                     if line == "": return
                     if b > eB: yield line[:len(line)-(b-eB)]; return
                     yield line[:-1] if line[-1] == "\n" else line
     except FileNotFoundError: pass
 def _catGenBin(fn, sB, eB):
     chunkSize = settings.cat.chunkSize; sB = wrap(fn, sB); eB = wrap(fn, eB); nB = eB - sB # number of bytes to read total
-    with open(fn, "rb") as f:
+    with openFile(fn, False) as f:
         f.seek(sB); nChunks = math.ceil(nB / chunkSize); lastChunkSize = nB - chunkSize*(nChunks-1)
         yield from range(nChunks) | cli.applyTh(lambda i: f.read(chunkSize) if i < nChunks-1 else f.read(chunkSize)[:lastChunkSize], prefetch=10)
 def fileLength(fn):
-    with open(fn, 'rb') as f: return f.seek(0, os.SEEK_END)
+    with openFile(fn, False) as f: return f.seek(0, os.SEEK_END)
 def wrap(fn, b): return b if b >= 0 else b + fileLength(fn) + 1
 class _cat(BaseCli):
     def __init__(self, text, chunks, sB, eB): self.text = text; self.chunks = chunks; self.sB = sB; self.eB = eB
     def _typehint(self, ignored=None):
         if self.text: return tIter(str) if self.chunks else tList(str)
         else: return tIter(bytes) if self.chunks else bytes
     def __ror__(self, fn:str) -> Union[Iterator[str], bytes]:
         text = self.text; chunks = self.chunks; sB = self.sB; eB = self.eB
         fn = os.path.expanduser(fn)
         if text and chunks and k1lib._settings.packages.k1a:
             return k1lib._k1a.k1a.StrIterCat(fn, sB, eB)
         if chunks: return _catGenText(fn, sB, eB) if text else _catGenBin(fn, sB, eB)
         sB = wrap(fn, sB); eB = wrap(fn, eB)
         if text:
-            with open(fn) as f: f.seek(sB); return f.read(eB-sB).splitlines()
+            with openFile(fn, True) as f: f.seek(sB); return f.read(eB-sB).splitlines()
         else:
-            with open(fn, "rb") as f: f.seek(sB); return f.read(eB-sB)
+            with openFile(fn, False) as f: f.seek(sB); return f.read(eB-sB)
 class Profile(BaseCli):
     def __init__(self, text): self.data = []; self.text = text
     def __ror__(self, it):
         fmt = k1lib.fmt; chars = 0; beginTime = time.time()
         if self.text:
             a, b, c, d, f = k1lib.ConstantPad.multi(5); every = settings.cat.every.text
             for lines, e in enumerate(it):
@@ -92,14 +194,20 @@
     # returns ['3456', '8']
     cat("test/catTest.pth", sB=2, eB=8) | deref()
     
     settings.cat.context.chunkSize=3 # just for demonstration, don't do it normally
     # returns [b'123', b'456', b'\\n8']
     cat("test/catTest.pth", text=False, chunks=True, eB=8) | deref()
 
+You can also read from urls directly, like this::
+
+    cat("https://k1lib.com/latest/") | deref()
+
+For remote files like this, there are extra settings at :data:`~k1lib.settings`.cli.RemoteFile
+
 If you are working with large files and would like to read 1 file from multiple
 threads/processes, then you can use this cli in conjunction with :class:`splitSeek`.
 
 If you are dumping multiple pickled objects into a single file, you can read all
 of them using :meth:`cat.pickle`.
 
 This cli has lots of settings at :data:`~k1lib.settings`.cli.cat
@@ -164,15 +272,18 @@
 using :meth:`cat`, like this::
 
     # returns [['  0_56789', '  1_56789', '  2_56789'], ['  3_56789', '  4_56789', '  5_56789', '  6_56789']]
     "test/largeFile.txt" | splitSeek(31) | splitSeek.window() | ~apply(lambda sB, eB: cat("test/largeFile.txt", sB=sB, eB=eB)) | head(2) | deref()
 
 Because :math:`120/31\\approx4`, most of cat's reads contain 4 lines, but some
 has 3 lines. Also notice that the lines smoothly transitions between cat's
-reads (``2_56789`` to ``3_56789``), so that's pretty nice.
+reads (``2_56789`` to ``3_56789``), so that's pretty nice. Just like with :meth:`cat`,
+this also works with urls::
+
+    "https://example.com" | splitSeek(10)
 
 .. warning::
 
     You have to really test whether reading the same file from multiple processes is
     going to be really faster or not. If your data is stored in a HDD (aka hard drive,
     with spinning disks), then it will actually slow you down (10x-100x), because the
     disk will have to context switch all the time, and each switch has a 10ms cost.
@@ -193,15 +304,16 @@
     CTGGAATTTGGTATCTTATTGCCAAAGAATCTGTTTTGTGAAACTTGGGATCTCTATTTTAATGTTAATTCTGGTCAGTTGTGCCTAAACTCCATAAAGCAGGGACTATACTGAGGCGTATTCAATCTTCCTTCTTACCAAGGCCAGGAA
     +
     EEFECEDEFFCGFFFFFEEEGEGFEDECCEFEFDFEEFDFEDDFEFEEFDDFFEEFFEEFEFFHEEFEEFEFFDEFFFECF>FFFEFEDFCFFFEGFEDEEGDDFEEFEFGEEBD@EG>EEFFECEEGFEEEFFEDGEEEDE5EBDG:CC
 
 Here, each 4 lines are (title, read, blank, quality). Because by default, this will
 only split neatly along new lines, you will have to write extra functions to detect
 if a particular seek point is desirable, and if not, either jump forward or backward
-using :meth:`splitSeek.forward` and :meth:`splitSeek.backward`.
+using :meth:`splitSeek.forward` and :meth:`splitSeek.backward`. To help with this,
+:class:`refineSeek` has some useful methods that you might want to check out.
 
 :param n: how many splits do you want?
 :param c: block-boundary character, usually just the new line character
 :param ws: weights. If given, the splits length ratios will roughly correspond to this"""
         self.n = n; self.c = c; self.ws = ws; self.fn = None; self.res = None # file name, result
         if ws is None and n is None: raise Exception("Specify at least n or ws for splitSeek to work")
     @staticmethod
@@ -221,15 +333,15 @@
         def inner(f):
             f.seek(i)
             while True:
                 b = f.tell(); s = f.read(1000); di = s.find(c)
                 if di > -1: return b + di + 1
                 if s == "": return -1
         if isinstance(f, str):
-            with open(os.path.expanduser(f), "rb") as _f: return inner(_f)
+            with openFile(os.path.expanduser(f), False) as _f: return inner(_f)
         else: return inner(f)
     @staticmethod
     def backward(f, i:int, c=b'\n') -> int:
         """Returns char location after the search char, going backward.
 Example::
 
     f = io.BytesIO(b"123\\n456\\n789\\nabc")
@@ -241,31 +353,30 @@
 :param f: file handle
 :param i: current seek point
 :param c: block-boundary character"""
         def inner(f):
             mul = 1
             while True:
                 begin = max(i-1000*mul, 0); end = max(i-1000*(mul-1), 0); mul += 1 # search range
-                f.seek(begin); b = f.tell()
-                s = f.read(end-begin); di = s.rfind(c)
+                f.seek(begin); b = f.tell(); s = f.read(end-begin); di = s.rfind(c)
                 if di > -1: return b + di + 1
                 if b == 0: return 0
         if isinstance(f, str):
-            with open(os.path.expanduser(f), "rb") as _f: return inner(_f)
+            with openFile(os.path.expanduser(f), False) as _f: return inner(_f)
         else: return inner(f)
     def __ror__(self, fn): # why return self instead of the seek positions directly? Because we want to pass along dependencies like file name to downstream processes like refineSeek
         if isinstance(fn, str): fn = os.path.expanduser(fn)
         n = self.n; c = self.c; ws = self.ws; self.fn = fn
         def func(f):
             f.seek(0, os.SEEK_END); end = f.tell()
             if ws is None: begins = range(n) | cli.apply(lambda x: int(x*end/n))
             else: begins = range(end) | cli.splitW(*ws) | cli.apply(lambda x: x.start)
             return [*begins | cli.apply(lambda x: splitSeek.backward(f, x, c)), end]
         if isinstance(fn, str):
-            with open(os.path.expanduser(fn), 'rb') as f: self.res = func(f); return self
+            with openFile(os.path.expanduser(fn), False, True) as f: self.res = func(f); return self
         else: self.res = func(fn); return self
     def __or__(self, aft):
         if self.res is None: return super().__or__(aft)
         return aft.__ror__(self)
     def __getitem__(self, idx): return self.res[idx]
     def __len__(self): return len(self.res)
     def __iter__(self): return iter(self.res)
@@ -311,22 +422,22 @@
         def read(fio, sB:int):
             fio.seek(sB)
             if window == 1: return fio.readline()
             return list(cli.repeatF(lambda: fio.readline(), window))
         if len(seeks) <= 2: return seeks
         fn = self.fn or seeks.fn; newSeeks = [seeks[0]]
         def process(fio):
-            with open(fn, "rb") as fio:
+            with openFile(fn, False) as fio:
                 for seek in seeks[1:-1]:
                     line = read(fio, seek)
                     while not f(line): seek = splitSeek.forward(fio, seek); line = read(fio, seek)
                     newSeeks.append(seek)
             newSeeks.append(seeks[-1]); return newSeeks
         if isinstance(fn, str):
-            with open(fn, "rb") as fio: return process(fio)
+            with openFile(fn, False) as fio: return process(fio)
         else: return process(fn)
     @classmethod
     def fastq(cls):
         """Refine fastq file's seek points"""
         return cls(lambda x: x[0][0] == b"@"[0] and x[2][0] == b"+"[0], 3)
 def curl(url:str, tries=3) -> Iterator[str]:
     """Gets file from url. File can't be a binary blob.
@@ -398,15 +509,15 @@
     if inp is not None:
         if isinstance(inp, (str, bytes)): p.stdin.write(inp if isinstance(inp, bytes) else inp.encode())
         else:
             for e in inp:
                 if not isinstance(e, (str, bytes)): e = str(e)
                 if not isinstance(e, bytes): e = e.encode()
                 p.stdin.write(e); p.stdin.write(b"\n")
-    p.stdin.close(); return lazySt(p.stdout, text), lazySt(p.stderr, text)
+    p.stdin.close(); return p, lazySt(p.stdout, text), lazySt(p.stderr, text)
 def printStderr(err):
     if not k1lib.settings.cli.quiet:
         e, it = err | cli.peek()
         if it != []: it | cli.insert("\nError encountered:\n") | cli.apply(k1lib.fmt.txt.red) | cli.stdout()
 def requireCli(cliTool:str):
     """Searches for a particular cli tool (eg. "ls"), throws ImportError if not
 found, else do nothing"""
@@ -495,15 +606,15 @@
     def _typehint(self, ignored=None):
         t = tIter(str) if self.text else tIter(bytes)
         if self.mode == 0: return tCollection(t, t)
         return t
     def __ror__(self, it:Union[None, str, bytes, Iterator[Any]]) -> Iterator[Union[str, bytes]]:
         """Pipes in lines of input, or if there's nothing to
 pass, then pass None"""
-        if not self.ro.done(): self.out, self.err = executeCmd(self.cmd, it, self.text); mode = self.mode
+        if not self.ro.done(): self.p, self.out, self.err = executeCmd(self.cmd, it, self.text); mode = self.mode
         if self.block:
             self.out = self.out | cli.deref()
             self.err = self.err | cli.deref()
         if mode == 0: return (self.out, self.err)
         elif mode == 1:
             threading.Thread(target=lambda: printStderr(self.err)).start()
             return self.out
```

## k1lib/cli/modifier.py

```diff
@@ -378,15 +378,15 @@
 def removePw():
     while True: time.sleep(settings.applyCl.sudoTimeout); _password.value = None
 t = threading.Thread(target=removePw, daemon=True).start()
 _nodeIdsCache = k1lib.Wrapper([])
 def specificNode(obj, nodeId:str):
     return obj.options(scheduling_strategy=ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy(node_id=nodeId, soft=False))
 class applyCl(BaseCli):
-    def __init__(self, f, prefetch=None, timeout=60, bs=1, rss:Union[dict, str]={}, pre:bool=False, orPatch=True, num_cpus=1, **kwargs):
+    def __init__(self, f, prefetch=None, timeout=60, bs=1, rss:Union[dict, str]={}, pre:bool=False, orPatch=True, num_cpus=1, resolve=True, **kwargs):
         """Like :class:`apply`, but execute a function over the input iterator
 in multiple processes on multiple nodes inside of a cluster (hence "cl"). So, just a more
 powerful version of :class:`applyMp`, assuming you have a cluster to run it on.
 Example::
 
     # returns [3, 2]
     ["abc", "de"] | applyCl(len) | deref()
@@ -467,14 +467,16 @@
     efficient.
 :param rss: resources required for the task. Can be {"custom_resource1": 2} or "custom_resource1" as a shortcut
 :param pre: "preserve", same convention as :meth:`applyCl.aS`. If True, then allow passing
     through node ids as the first column to shedule jobs on those specific nodes only
 :param orPatch: whether to automatically patch __or__ function so that cli tools can
     work with numpy arrays on that remote worker
 :param num_cpus: how many cpu does each task take?
+:param resolve: whether to resolve the outputs or not. Set this to False to not move
+    memory to the requesting node and cache the big data structure on the remote node
 :param kwargs: extra arguments to be passed to the function. ``args`` not
     included as there're a couple of options you can pass for this cli."""
         super().__init__(fs=[f]); _fC = fastF(f); self.ogF = f; self.pre = pre
         self.rss = rss = {rss: 1} if isinstance(rss, str) else rss
         def remoteF(e):
             if orPatch:
                 import k1lib; k1lib.cli.init.patchNumpy()
@@ -486,15 +488,15 @@
         self._copyCtx = lambda: [f, [prefetch, timeout, bs, rss, pre, orPatch, num_cpus], kwargs]
         def preprocessF(f, e): # return future (if pre=False), or [nodeId, future] (if pre=True)
             if pre: nodeId, e = e; return [nodeId, specificNode(f, nodeId).remote(e)]
             else: return f.remote(e)
         def resolveF(e):
             if pre: return [e[0], ray.get(e[1], timeout=timeout)]
             else: return ray.get(e, timeout=timeout)
-        self.preprocessF = preprocessF; self.resolveF = resolveF
+        self.preprocessF = preprocessF; self.resolveF = resolveF if resolve else cli.iden()
     def __ror__(self, it):
         f = self.f; timeout = self.timeout; bs = self.bs; ogF = self.ogF; preprocessF = self.preprocessF; resolveF = self.resolveF
         if bs > 1: return it | cli.batched(bs, True) | applyCl(lambda x: x | apply(ogF) | cli.aS(list), self.prefetch, timeout) | cli.joinStreams()
         def gen(it):
             futures = deque(); it = iter(it)
             for i, e in zip(range(self.prefetch), it): futures.append(preprocessF(f, e))
             for e in it: yield resolveF(futures.popleft()); futures.append(preprocessF(f, e))
@@ -529,15 +531,15 @@
         """Grabs the metadata object for the current node"""
         return ray.nodes() | cli.filt(lambda x: x["NodeID"] == applyCl.nodeId()) | cli.item()
     @staticmethod
     def cpu() -> int:
         """Grabs the number of cpus available on this node"""
         return int(applyCl.meta()["Resources"]["CPU"])
     @staticmethod
-    def aS(f, timeout:float=8):
+    def aS(f, timeout:float=8, resolve:bool=True):
         """Executes function f once for all node ids that are piped in.
 Example::
 
     # returns [['1051da...', ['Desktop', 'Downloads']], ['7bb387...', ['Pictures', 'Music']]]
     applyCl.nodeIds() | applyCl.aS(lambda: None | cmd("ls ~") | deref()) | deref()
     # also returns [['1051da...', ['Desktop', 'Downloads']], ['7bb387...', ['Pictures', 'Music']]]
     None | applyCl.aS(lambda: None | cmd("ls ~") | deref()) | deref()
@@ -545,17 +547,18 @@
 If you want to execute f for all nodes, you can pass in None instead.
 
 As a reminder, this kinda follows the same logic as the popular cli :class:`aS`, where
 f is executed once, hence the name "apply Single". Here, the meaning of "single" is
 different. It just means execute once for each node ids.
 
 :param f: main function to execute in each node. Not supposed to accept any arguments
-:param timeout: seconds to wait for job before raising an error"""
+:param timeout: seconds to wait for job before raising an error
+:param resolve: whether to resolve the result or not. See main docs at :class:`applyCl`"""
         f = fastF(f); g = lambda nodeId: specificNode(ray.remote(f), nodeId).remote()
-        final = cli.iden() & (apply(g) | aS(list) | apply(ray.get, timeout=timeout)) | cli.transpose()
+        final = cli.iden() & (apply(g) | aS(list) | (apply(ray.get, timeout=timeout) if resolve else cli.iden())) | cli.transpose()
         def inner(it):
             if it is None: it = applyCl.nodeIds()
             else:
                 if it | ~cli.inSet(_nodeIdsCache()) | cli.shape(0) > 0:
                     _nodeIdsCache.value = applyCl.nodeIds(); outliers = it | ~cli.inSet(_nodeIdsCache()) | cli.deref()
                     if len(outliers) > 0: raise Exception(f"These nodes cannot be found: {outliers}")
             return it | final
@@ -683,20 +686,21 @@
 :param nBs: node ids that will store the file after balancing everything out. If not
     specified, will take all available nodes
 :param rS: :class:`~k1lib.cli.inp.refineSeek` instance, if you need more fine-grained
     control over section boundaries so as to not make everything corrupted
 """
         from k1lib.cli._applyCl import balanceFile
         balanceFile(fn, nAs, nBs, rS or cli.iden())
-    def decommission(self, fn, nAs:List[str], rS=None):
+    @staticmethod
+    def decommissionFile(fn, nAs:List[str], rS=None):
         """Convenience function for :meth:`balanceFile`. See docs over there."""
         from k1lib.cli._applyCl import balanceFile
-        balanceFile(fn, None, applyCl.nodeIds() | ~cli.inSet(nAs) | deref(), rS or cli.iden())
+        balanceFile(fn, None, applyCl.nodeIds() | ~cli.inSet(nAs) | cli.deref(), rS or cli.iden())
     @staticmethod
-    def cat(fn:str=None, f:Callable=None, timeout:float=60, keepNodeIds:bool=False, multiplier:int=1, includeId:bool=False):
+    def cat(fn:str=None, f:Callable=None, nodeIds=None, timeout:float=60, keepNodeIds:bool=False, multiplier:int=1, includeId:bool=False):
         """Reads a file distributedly, does some operation on them, collects and
 returns all of the data together. Example::
 
     fn = "~/repos/labs/k1lib/k1lib/cli/test/applyCl.cat.data"
     ("0123456789"*5 + "\\n") * 1000 | file(fn)
     applyCl.splitFile(fn)
     applyCl.cat(fn, shape(0), keepNodeIds=True) | deref()
@@ -749,14 +753,15 @@
     it too much? Probably, but good thing is that you can pick any that's intuitive
     for you. Note that this mode is just for convenience only, for when you want to do
     exploratory analysis on a single remote file. To be efficient at bulk processing,
     use the normal mode instead.
 
 :param fn: file name
 :param f: function to execute in every process
+:param nodeIds: only read file from these nodes
 :param timeout: kills the processes if it takes longer than this amount of seconds
 :param keepNodeIds: whether to keep the node id column or not
 :param multiplier: by default, each node will spawn as many process as there
     are cpus. Sometimes you want to spawn more process, change this to a higher number
 :param includeId: includes a unique id for this process
 """
         if f is None: # simple case
@@ -765,15 +770,15 @@
                 inter = seeks | cli.window(2) | apply(cli.wrapList() | cli.insert(nodeId)) | cli.deref()
                 return inter | ~applyCl(lambda sB, eB: cli.cat(fn,sB=sB,eB=eB) | cli.deref(), pre=True) | cli.cut(1) | cli.joinStreams()
                 # return [nodeId_fn] | applyCl(cat() | deref(), pre=True) | cut(1) | item() # direct, no chunking method
             if fn is None: return aS(inner) # [nodeId, fn] | applyCl.cat()
             if isinstance(fn, str): return aS(lambda nodeId: inner([nodeId, fn])) # nodeId | applyCl.cat()
             else: return inner(fn) # applyCl.cat([nodeId, fn])
         postprocess = cli.insertIdColumn(True, False) | ~apply(lambda x,y,z: [x,[*y,z]])
-        checkpoints = None | applyCl.aS(lambda: fn | cli.splitSeek(int(applyCl.meta()["Resources"]["CPU"]*multiplier)) | cli.window(2) | cli.deref()) | cli.ungroup() | postprocess | cli.deref()
+        checkpoints = nodeIds | applyCl.aS(lambda: fn | cli.splitSeek(int(applyCl.meta()["Resources"]["CPU"]*multiplier)) | cli.window(2) | cli.deref()) | cli.ungroup() | postprocess | cli.deref()
         postprocess = cli.iden() if keepNodeIds else cli.cut(1)
         return checkpoints | applyCl(~aS(lambda x,y,idx: cli.cat(fn, sB=x, eB=y) | ((cli.wrapList() | cli.insert(idx)) if includeId else cli.iden()) | f), pre=True, timeout=timeout, num_cpus=1) | postprocess
     @staticmethod
     def balanceFolder(folder:str, maxSteps:int=None, audit:bool=False):
         """Balances all files within a folder across all nodes.
 Example::
 
@@ -801,14 +806,23 @@
 then rebalance the folders and do your analysis.
 
 :param folder: folder to rebalance all of the files
 :param maxSteps: what's the maximum number of file transfers? By default has no limit, so that files are transferred until 
 :param audit: if True, don't actually move files around and just return what files are going to be moved where"""
         from k1lib.cli._applyCl import balanceFolder
         return balanceFolder(folder, audit, maxSteps)
+    def decommissionFolder(folder:str, nAs:List[str], maxSteps:int=10000, audit:bool=False, timeout:float=3600):
+        """Like :meth:`decommissionFile`, but works for distributed folders instead.
+
+:param nAs: list of node ids to migrate files away from
+:param maxSteps: limits the total number of optimization steps. Normally don't have to specify,
+    but just here in case it runs for too long trying to optimize the folder structure
+:param audit: if True, just returns the file movements it's planning to do"""
+        from k1lib.cli._applyCl import decommissionFolder
+        return decommissionFolder(folder, nAs, audit=audit, maxSteps=maxSteps, timeout=timeout)
     def download(url:str, folder:str, merge:bool=False, timeout=120, chunkTimeout=5):
         """Downloads a file distributedly to a specified folder.
 Example::
 
     url = "https://vim.kelvinho.org"
     fn = "~/repos/labs/k1lib/k1lib/cli/test/applyCl.download" # file/folder name
     applyCl.download(url, fn)       # will download distributedly and dump file fragments into the folder fn
```

## k1lib/cli/output.py

```diff
@@ -39,14 +39,16 @@
 specified stream, while yielding the elements. Example::
 
     # prints "0\\n1\\n2\\n3\\n4\\n" and returns [0, 1, 4, 9, 16]
     range(5) | tee() | apply(op() ** 2) | deref()
 
 See also: :class:`~k1lib.cli.modifier.consume`
 
+This cli is not exactly well-thoughout and is a little janky
+
 :param f: element transform function. Defaults to just adding a new
     line at the end
 :param s: stream to write to. Defaults to :attr:`sys.stdout`
 :param every: only prints out 1 line in ``every`` lines, to limit print rate"""
         self.s = s or sys.stdout; self.f = f; self.every = every
     def __ror__(self, it):
         s = self.s; f = self.f; every = self.every
@@ -65,14 +67,19 @@
         """Like :meth:`tee.cr`, but includes an elapsed time text at the end.
 Example::
 
     range(5) | tee().cr() | apply(op() ** 2) | deref()"""
         beginTime = time.time()
         f = (lambda x: x) if self.f == _defaultTeeF else self.f
         self.f = lambda s: f"\r{f(s)}, {int(time.time() - beginTime)}s elapsed"; return self
+    def autoInc(self):
+        """Like :meth:`tee.crt`, but instead of printing the object, just print
+the current index and time"""
+        beginTime = time.time(); autoInc = k1lib.AutoIncrement()
+        self.f = lambda s: f"\r{autoInc()}, {int(time.time()-beginTime)}s elapsed"; return self
 try:
     import PIL; hasPIL = True
 except: hasPIL = False
 class file(BaseCli):
     def __init__(self, fileName:str=None, flush:bool=False, mkdir:bool=False):
         """Opens a new file for writing. This will iterate through
 the iterator fed to it and put each element on a separate line. Example::
```

## Comparing `k1lib-1.3.8.7.data/data/k1lib/k1ui/256.model.state_dict.pth` & `k1lib-1.3.9.data/data/k1lib/k1ui/256.model.state_dict.pth`

 * *Files identical despite different names*

## Comparing `k1lib-1.3.8.7.data/data/k1lib/k1ui/mouseKey.pth` & `k1lib-1.3.9.data/data/k1lib/k1ui/mouseKey.pth`

 * *Files identical despite different names*

## Comparing `k1lib-1.3.8.7.data/data/k1lib/serve/main.html` & `k1lib-1.3.9.data/data/k1lib/serve/main.html`

 * *Files identical despite different names*

## Comparing `k1lib-1.3.8.7.dist-info/LICENSE` & `k1lib-1.3.9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `k1lib-1.3.8.7.dist-info/METADATA` & `k1lib-1.3.9.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 Metadata-Version: 2.1
 Name: k1lib
-Version: 1.3.8.7
+Version: 1.3.9
 Summary: Some nice ML overhaul
 Home-page: https://k1lib.com
 Author: Quang Ho
 Author-email: 157239q@gmail.com
 License: MIT
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: numpy (>=1.14)
 Requires-Dist: matplotlib (>=2.0)
 Requires-Dist: dill
 Requires-Dist: forbiddenfruit
 Requires-Dist: wurlitzer
+Requires-Dist: validators
 Provides-Extra: all
 Requires-Dist: graphviz ; extra == 'all'
 Requires-Dist: torchvision ; extra == 'all'
 Requires-Dist: pillow ; extra == 'all'
 Requires-Dist: scikit-image ; extra == 'all'
 Requires-Dist: pyperclip ; extra == 'all'
 Requires-Dist: k1a (<2,>=1.2) ; extra == 'all'
```

## Comparing `k1lib-1.3.8.7.dist-info/RECORD` & `k1lib-1.3.9.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 k1lib/__init__.py,sha256=zdjKwxgRuNNLKhHen5zaW-zmo6DmOlGBR9frAQI0jh0,1435
-k1lib/_baseClasses.py,sha256=0uR1wDKqLHP2FJmMqb8ao3M-gGMHzyJ85UzUxtpUqGI,53793
+k1lib/_baseClasses.py,sha256=bwUkhuJcp5RJMJxHuDcGTF5cwenBFZIqouIluChPtB0,53798
 k1lib/_basics.py,sha256=f9Hn4UTRfhbzKotZ9eRMmb4Ic8A4_1ORP0qWnInkLu8,13494
 k1lib/_context.py,sha256=wZB8OhBZiJNgfk-CA9tVWmhyC38J-6vVXSn1vEIkCtE,4908
 k1lib/_higher.py,sha256=BZ54rlMNygGrKBZjLpakicQ_FDSt936X5sgfJnvns2Y,2866
 k1lib/_k1a.py,sha256=i-_CPMBdiE-CCI375eaguwcC01dGHar4SlaaQDFsRwg,948
 k1lib/_learner.py,sha256=63aPrhGE5jO1YApvsIcn_HXA1E4ttLfyZVkIUE-meqA,11646
 k1lib/_monkey.py,sha256=ojty9WZTOcW0NVajMLuL_5Dn_1523i5ONvok3m8hbGU,21467
 k1lib/_perlin.py,sha256=sL4B8caVEz4xgWj65Oy3FYxELGDM8wPAtsn0Ro8R3dc,3571
@@ -42,43 +42,43 @@
 k1lib/callbacks/lossFunctions/shorts.py,sha256=wXeUSgGDIdu_nsiAvn4pKs3fQrcO2FwpNXKcBCFvGzY,3465
 k1lib/callbacks/profilers/__init__.py,sha256=Gp5IvLRABYAg1J0ilTT2v72gfDbyTvUUHUEpvlSo1Lc,45
 k1lib/callbacks/profilers/computation.py,sha256=gPNsoioghh4PI5w8s2p8qYOrR7XObslqCLH5EkNOPSI,5054
 k1lib/callbacks/profilers/io.py,sha256=H8E0YzmLWRD9T2_RyDG2eXvbQQnJgzEnEyx8PqfQ4wY,2319
 k1lib/callbacks/profilers/memory.py,sha256=L0F5pc5LB0dtSnRot8ReR-amZn1uIXv0py0XmGS166U,4419
 k1lib/callbacks/profilers/time.py,sha256=R2-2ZooDwLQIeyonLp2Zz5E_uXzdy6mWUgw6uavQbpE,4215
 k1lib/cli/__init__.py,sha256=hF0ODhL20OSM9o1j68VhcIVflibgSpPuqeyYlsh8oow,925
-k1lib/cli/_applyCl.py,sha256=tq-2axIX-KM-ZCR8z3eqL5amSbPNg3HvR-Y6pJuzyw8,15774
+k1lib/cli/_applyCl.py,sha256=4nzRGnYdlUntCl7HmG-tUKj63w6hmIO0InL7lb2tpX4,19471
 k1lib/cli/bio.py,sha256=PhGvy-fDA-wrUzzEDpuRe4x-Kbylx0sNmoXCEZfE_FA,8308
 k1lib/cli/cif.py,sha256=77FX83m1FRYEeZkdXJ8MiVapqCSzZ-1xOQ8ZLeHfhf8,4033
 k1lib/cli/conv.py,sha256=KcwSs9mD54XA5BC7ajxPF_1CZsu54i6dH6DtEv_fbv0,19321
 k1lib/cli/filt.py,sha256=c-bueTrH_5KAgRTtNiK4t4jfCy6xu-d0rYEUVF9KKGA,24071
 k1lib/cli/gb.py,sha256=xxjuNYgWrrElRckon3gP0sj-dShYnKs3jmHAb1U0kVI,6672
 k1lib/cli/grep.py,sha256=Lu4PFOe2pkaqd-UfJe_HhHCUFTUifE6Bh96_k80sQDA,6348
 k1lib/cli/init.py,sha256=2FaWeRqZnb--XWV4Xpo0z4ANDdyWpNQlHKkALtOGHKQ,18351
-k1lib/cli/inp.py,sha256=BRwyBjLmo6DZbl0S3XZzKv5hnA27lDHgW6jJPSIYXGE,25961
+k1lib/cli/inp.py,sha256=uj44IMxiisy8M6kt7mekPl_M8Evn3V24j32mHybOIRE,32491
 k1lib/cli/kcsv.py,sha256=YGUVVLTZGGujokhxtj5MfjU9t1jRGqp23d58JK8lhq0,623
 k1lib/cli/kxml.py,sha256=YQGutvKNm0_xAi_NhCNtuGey7fx3zZSmSo33kS--54c,4819
 k1lib/cli/mgi.py,sha256=aLke90nG89tgWLPwyKmTj3kM8yJnIBCJSrPS1jT8mUk,1915
-k1lib/cli/modifier.py,sha256=c9NNvERa4xGxRMNHOaA-oVo19iKlWuXY1c-fW6gUkpQ,63021
+k1lib/cli/modifier.py,sha256=pizUolw86BuAAXV6ykjBPw1_MkM27ezlqCJOVINwLPc,64124
 k1lib/cli/mol.py,sha256=wNFuCPXtdEcH4DRBbmYaLAWxtDzjN2MOKFX7ynJhaJs,694
 k1lib/cli/nb.py,sha256=LsNN7OFJ6KzAYKvZpm4fj9WRpsX6Srx6D_xpSTCV328,4038
 k1lib/cli/optimizations.py,sha256=iZ73DwLqZCxRm0sECVZ7A2nDxf5D4rsoSGzrKTgzGaI,3530
-k1lib/cli/output.py,sha256=JVOMyvvNmdf7CTpK8cTZesaJL4EDWZECpcDvg-NtHhQ,11762
+k1lib/cli/output.py,sha256=sGk7Z_kiJ0A_wwRtlJZZgvfqvbl1K-XmNsGih0PlqeA,12118
 k1lib/cli/sam.py,sha256=_ersEPP2ue0Oa3AyftNjQu2PABpH4L7iFBbRJDOkeug,2394
 k1lib/cli/structural.py,sha256=Ugn0Fb-0n0ardjjAvlfsfhRehLeUAszjvkW1odv6ZDU,49390
 k1lib/cli/trace.py,sha256=nzZgOyXqFJYkQfbpR0lpX0Nnp0bQHXPjk8sDUBIe2hk,10399
 k1lib/cli/typehint.py,sha256=VBYxrOQaSnu5266lNWpgXIhXF7htdT0FT_NEvXjYBVk,23319
 k1lib/cli/utils.py,sha256=XF-H1Ce5ssOw6Dp3htiosHce7xxTgxptrlUsMYxI7uE,21101
 k1lib/k1ui/__init__.py,sha256=8D5a8oKgqd6WA1RUkiKCn4l_PVemtyuckxQut0vDHXM,20
 k1lib/k1ui/main.py,sha256=PnmdOhkjYgRSZnDyGYNMYtQ5Nvcb1NhQ9yjfP_3QORI,61803
 k1lib/serve/__init__.py,sha256=8D5a8oKgqd6WA1RUkiKCn4l_PVemtyuckxQut0vDHXM,20
 k1lib/serve/main.py,sha256=Xh2SzgABfsBp2dRLUJRMftsG_We8ReVHYqXLi3ntMVA,10361
 k1lib/serve/suffix-dash.py,sha256=HMNJvB4d-PTHXDRDQTdYUKtzgirJ0LVnqqAkXxO0B4w,153
 k1lib/serve/suffix.py,sha256=UH3ITN6O2vzoha2f6v4bcQG3_Boav7VA7EC8wf8r9f8,642
-k1lib-1.3.8.7.data/data/k1lib/k1ui/256.model.state_dict.pth,sha256=Ga-UXlAJfUPNOZsvP_c1-1cfB2Hp_-oQ4ghdouX1d7g,2453826
-k1lib-1.3.8.7.data/data/k1lib/k1ui/mouseKey.pth,sha256=KULhK_gdK2Ppju9gQnv1zV2kf_A0K-vX2W7trY6DIg8,304735
-k1lib-1.3.8.7.data/data/k1lib/serve/main.html,sha256=gHFNqzE9JKb4eCwCJN-Du45jj75lDRED4Ico91T-b4g,20544
-k1lib-1.3.8.7.dist-info/LICENSE,sha256=psuy2wnTg9zacuCQ0dXfxS44iaa89aTgsNzHDzx4UGM,1049
-k1lib-1.3.8.7.dist-info/METADATA,sha256=RM3SL_e6mPqw-myE63EF_VYKikyn4sSS_U1_Dye7GQU,3864
-k1lib-1.3.8.7.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-k1lib-1.3.8.7.dist-info/top_level.txt,sha256=xxWmqZzuThnLZn49Mse6A6j41-IVduPVnQW54imcOTA,6
-k1lib-1.3.8.7.dist-info/RECORD,,
+k1lib-1.3.9.data/data/k1lib/k1ui/256.model.state_dict.pth,sha256=Ga-UXlAJfUPNOZsvP_c1-1cfB2Hp_-oQ4ghdouX1d7g,2453826
+k1lib-1.3.9.data/data/k1lib/k1ui/mouseKey.pth,sha256=KULhK_gdK2Ppju9gQnv1zV2kf_A0K-vX2W7trY6DIg8,304735
+k1lib-1.3.9.data/data/k1lib/serve/main.html,sha256=gHFNqzE9JKb4eCwCJN-Du45jj75lDRED4Ico91T-b4g,20544
+k1lib-1.3.9.dist-info/LICENSE,sha256=psuy2wnTg9zacuCQ0dXfxS44iaa89aTgsNzHDzx4UGM,1049
+k1lib-1.3.9.dist-info/METADATA,sha256=g15QLqmHRJJnbKlYp93PENLkdecyvMROIKqbQ2M2IHE,3888
+k1lib-1.3.9.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+k1lib-1.3.9.dist-info/top_level.txt,sha256=xxWmqZzuThnLZn49Mse6A6j41-IVduPVnQW54imcOTA,6
+k1lib-1.3.9.dist-info/RECORD,,
```

